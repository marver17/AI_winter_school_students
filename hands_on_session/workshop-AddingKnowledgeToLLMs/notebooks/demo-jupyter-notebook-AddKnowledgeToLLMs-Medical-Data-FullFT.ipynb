{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5f9605e7-b538-469c-b88d-4876c3fadd66",
   "metadata": {},
   "source": [
    "# üß† Workshop: Adding Knowledge to LLMs  \n",
    "### Dataset: lavita/ChatDoctor-HealthCareMagic-100k  \n",
    "HuggingFace: https://huggingface.co/datasets/lavita/ChatDoctor-HealthCareMagic-100k  \n",
    "\n",
    "### Base Model: google/gemma-2-2b-it  \n",
    "HuggingFace: https://huggingface.co/google/gemma-2-2b-it  \n",
    "\n",
    "---\n",
    "\n",
    "## 1Ô∏è‚É£ Full Fine-Tuning (Full FT)\n",
    "\n",
    "In **Full Fine-Tuning**, all model parameters are updated during training.  \n",
    "This provides maximum domain adaptation but requires high GPU memory and compute.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4d40f0b2-d346-4cf1-bc65-cd2204e078c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Workshop: Adding Knowledge to LLMs\n",
    "# ============================================================\n",
    "# Dataset: lavita/ChatDoctor-HealthCareMagic-100k\n",
    "#         HuggingFace Dataset Link: https://huggingface.co/datasets/lavita/ChatDoctor-HealthCareMagic-100k\n",
    "\n",
    "# Model: google/gemma-2-2b-it\n",
    "#         HuggingFace Model Link: https://huggingface.co/google/gemma-2-2b-it\n",
    "\n",
    "# ============================================================\n",
    "# Goal:\n",
    "# - Fine-tune a model on Medical ChatDoctor Data using:\n",
    "# 1) Full Fine-Tuning\n",
    "# 2) LoRA\n",
    "# 3) QLoRA (4-bit + LoRA)\n",
    "# 4) Build a RAG baseline using the SAME data and Evaluate all approaches using the SAME questions\n",
    "# 5) Create a Medical Agent\n",
    "# ============================================================\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "64c5bb70-7171-48ef-9828-e53037d2d50b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/leonardo/home/userexternal/gcortiad/workshop-AddingKnowledgeToLLMs/notebooks\n"
     ]
    }
   ],
   "source": [
    "# =====================================================\n",
    "# Full Fine-Tuning\n",
    "# =====================================================\n",
    "# Check Current Path\n",
    "!pwd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd43c126-2bb6-4c10-83f8-ab92d561cb52",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### üì¶ Step 0: Environment Setup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "20a680a4-562d-45e4-ac54-c1edfbb31d2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/leonardo/home/userexternal/gcortiad/fine-tune-dev-jup-env-test/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# =====================================================\n",
    "# 0. Setup\n",
    "# =====================================================\n",
    "import os\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer, AutoModelForCausalLM, TrainingArguments, Trainer,\n",
    "    DataCollatorForLanguageModeling, BitsAndBytesConfig\n",
    ")\n",
    "from peft import LoraConfig, get_peft_model\n",
    "from trl import SFTTrainer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from utils.utils import get_gpu_memory, generate_chat_response\n",
    "import bitsandbytes as bnb\n",
    "import torch.nn as nn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "64e9faa4-97e6-4f5c-9e13-a161035b2531",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Environment Variables\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
    "os.environ[\"DATA_PATH\"] = \"/leonardo_work/tra26_minwinsc/workshop-AddingKnowledgeToLLMs/datasets/ChatDoctor-dataset/data/\"\n",
    "os.environ[\"MODEL_PATH\"] = \"/leonardo_work/tra26_minwinsc/workshop-AddingKnowledgeToLLMs/models/gemma-2-2b-it\"\n",
    "os.environ[\"FULL_FT_MODEL_PATH\"] = \"/leonardo_work/tra26_minwinsc/workshop-AddingKnowledgeToLLMs/FT-models/full_model_chatdoctor_gemma-2-2b-it/checkpoint-564/\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "04761d3b-0583-4842-a4db-fd7d13930c63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/leonardo/home/userexternal/gcortiad/workshop-AddingKnowledgeToLLMs/FT-models/test/full_model_chatdoctor_gemma-2-2b-it\n"
     ]
    }
   ],
   "source": [
    "# Check path where we want to store out Full Fine-tuned Model\n",
    "print(os.getenv(\"FULL_FT_MODEL_PATH\", None))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d0308113-3703-42bc-881b-c9c1431bc205",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mon Feb 23 12:41:28 2026       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 535.274.02             Driver Version: 535.274.02   CUDA Version: 12.6     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  NVIDIA A100-SXM-64GB           On  | 00000000:1D:00.0 Off |                    0 |\n",
      "| N/A   42C    P0              63W / 468W |      4MiB / 65536MiB |      0%      Default |\n",
      "|                                         |                      |             Disabled |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "|   1  NVIDIA A100-SXM-64GB           On  | 00000000:56:00.0 Off |                    0 |\n",
      "| N/A   43C    P0              62W / 473W |      4MiB / 65536MiB |      0%      Default |\n",
      "|                                         |                      |             Disabled |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "|   2  NVIDIA A100-SXM-64GB           On  | 00000000:8F:00.0 Off |                    0 |\n",
      "| N/A   43C    P0              61W / 460W |      4MiB / 65536MiB |      0%      Default |\n",
      "|                                         |                      |             Disabled |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "|   3  NVIDIA A100-SXM-64GB           On  | 00000000:C8:00.0 Off |                    0 |\n",
      "| N/A   42C    P0              61W / 459W |      4MiB / 65536MiB |      0%      Default |\n",
      "|                                         |                      |             Disabled |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "|  No running processes found                                                           |\n",
      "+---------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "# Check GPUs available \n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bb6be30c-4271-441a-9f90-918c7223caf7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'total_gb': 63.42, 'used_gb': 0.47, 'free_gb': 62.95, 'source': 'torch'}\n"
     ]
    }
   ],
   "source": [
    "gpu_mem = get_gpu_memory()\n",
    "print(gpu_mem)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe306225-9484-46a9-bb45-3b87541515b2",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### üì• Step 1: Load Dataset\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6a1cdec4-479d-491b-b3b0-3eb7b1464b3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =====================================================\n",
    "# 1. Load ChatDoctor Dataset\n",
    "# =====================================================\n",
    "# Load the dataset from the local directory\n",
    "chatdoctor = load_dataset(os.getenv(\"DATA_PATH\", None))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ee234dca-0a88-4add-a9f0-a74d73db32a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['instruction', 'input', 'output'],\n",
       "        num_rows: 112165\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chatdoctor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e619c0a2-cc4e-4009-95f1-c7045647926d",
   "metadata": {},
   "source": [
    "The dataset contains medical conversations between patients and doctors."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df8d2273-fbb3-429c-9880-257b0cbd31dd",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### üìÇ Step 2: Define Model Path and Load Tokenizer\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "04ac816c-f6a8-4e70-8bf1-4efd27c94c50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =====================================================\n",
    "# 2. Model Path & Tokenizer\n",
    "# =====================================================\n",
    "# Define the model we want to fine tune.\n",
    "model_path = os.getenv(\"MODEL_PATH\", None)\n",
    "model_name = str(model_path.split(\"/\")[-1])\n",
    "\n",
    "# Get Model Tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d86f3fbb-e776-4aa3-ba11-2d54cc2339fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model used for Full Fine-Tuning: gemma-2-2b-it\n"
     ]
    }
   ],
   "source": [
    "print(f\"Model used for Full Fine-Tuning: {model_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50344b60-afd9-40fd-8bed-7645d7ca4d8d",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### üßπ Step 3: Apply Chat Template to the Data + Tokenization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "421f5a30-3bf1-4ed5-9141-2ec839f53f6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =====================================================\n",
    "# 3. Apply Chat Template & Data Collator with Dynamic Padding\n",
    "# =====================================================\n",
    "def format_chat_template(row):\n",
    "    row_json = [\n",
    "        {\"role\": \"user\", \"content\": f\"INSTRUCTION:\\n{row['instruction']}\\n\\nPATIENT MESSAGE:\\n{row['input']}\"},\n",
    "        {\"role\": \"assistant\", \"content\": row[\"output\"]}\n",
    "    ]\n",
    "    row[\"text\"] = tokenizer.apply_chat_template(row_json, tokenize=False)\n",
    "    return row\n",
    "\n",
    "# Apply chat template to all data\n",
    "chatdoctor = chatdoctor.map(format_chat_template, num_proc=4)\n",
    "\n",
    "# Split Train and Test datasets\n",
    "split_dataset = chatdoctor['train'].train_test_split(\n",
    "    test_size=0.20,\n",
    "    seed=42,\n",
    "    shuffle=True,\n",
    ")\n",
    "train_dataset = split_dataset['train']\n",
    "val_dataset = split_dataset['test']\n",
    "\n",
    "# Define the Data Collator for creating batches of the data\n",
    "def data_collator(batch):\n",
    "    tokenized = tokenizer(\n",
    "        [example[\"text\"] for example in batch],\n",
    "        return_tensors=\"pt\",\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        max_length=2048,\n",
    "    )\n",
    "    # For causal LM, labels are just input_ids\n",
    "    tokenized[\"labels\"] = tokenized[\"input_ids\"].clone()\n",
    "    return tokenized\n",
    "\n",
    "# Subsample for workshop (select only X rows)\n",
    "train_data = train_dataset.select(range(3000)) #.shuffle(seed=42).select(range(2000)) # Shuffle before choosing X rows\n",
    "val_data = val_dataset.select(range(300))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d9ef2e3-03eb-4693-8381-124b5585cef3",
   "metadata": {},
   "source": [
    "---\n",
    "### ü§ñ Step 4: Load Gemma Model and Run the Full Fine Tuning\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d3b546a7-4a41-4c5a-aaf3-94545e9f1536",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:01<00:00,  1.17it/s]\n",
      "Adding EOS to train dataset: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3000/3000 [00:00<00:00, 12856.89 examples/s]\n",
      "Tokenizing train dataset: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3000/3000 [00:01<00:00, 1886.13 examples/s]\n",
      "Truncating train dataset: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3000/3000 [00:01<00:00, 2928.35 examples/s]\n",
      "Adding EOS to eval dataset: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 300/300 [00:00<00:00, 10456.48 examples/s]\n",
      "Tokenizing eval dataset: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 300/300 [00:00<00:00, 1822.99 examples/s]\n",
      "Truncating eval dataset: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 300/300 [00:00<00:00, 54696.42 examples/s]\n",
      "WARNING:accelerate.utils.other:Detected kernel version 4.18.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n",
      "The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'eos_token_id': 1}.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Allocated before training: 4.006139904 GB\n",
      "Reserved before training: 4.028628992 GB\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='564' max='564' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [564/564 26:05, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Entropy</th>\n",
       "      <th>Num Tokens</th>\n",
       "      <th>Mean Token Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>2.347100</td>\n",
       "      <td>1.738018</td>\n",
       "      <td>2.457841</td>\n",
       "      <td>421433.000000</td>\n",
       "      <td>0.639764</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>1.989800</td>\n",
       "      <td>1.793467</td>\n",
       "      <td>1.690795</td>\n",
       "      <td>840380.000000</td>\n",
       "      <td>0.647913</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>1.203400</td>\n",
       "      <td>1.816977</td>\n",
       "      <td>1.586412</td>\n",
       "      <td>1265617.000000</td>\n",
       "      <td>0.651943</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>1.024000</td>\n",
       "      <td>2.336069</td>\n",
       "      <td>1.012148</td>\n",
       "      <td>1688143.000000</td>\n",
       "      <td>0.644566</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.397100</td>\n",
       "      <td>2.519331</td>\n",
       "      <td>0.908757</td>\n",
       "      <td>2109549.000000</td>\n",
       "      <td>0.642725</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Peak Allocated during training: 20.903665664 GB\n",
      "Peak Reserved during training: 60.599304192 GB\n"
     ]
    }
   ],
   "source": [
    "# =====================================================\n",
    "# 4. Read Model + Full Fine-Tuning\n",
    "# =====================================================\n",
    "# Read Base Model\n",
    "model = AutoModelForCausalLM.from_pretrained(model_path, torch_dtype=torch.bfloat16, device_map='auto')\n",
    "\n",
    "# Define Training Arguments\n",
    "train_args = TrainingArguments(\n",
    "    per_device_train_batch_size=2,    # Try distinct values of batch_size\n",
    "    gradient_accumulation_steps=8,    # Try distinct values of gradients accumulation\n",
    "\n",
    "    num_train_epochs=3, \n",
    "    learning_rate=5e-5, #2e-4,        # Try distinct learning_rate values\n",
    "    \n",
    "    fp16=False,\n",
    "    bf16=True,\n",
    "    \n",
    "    logging_strategy=\"steps\",\n",
    "    logging_steps=100,\n",
    "    warmup_steps=30,\n",
    "    \n",
    "    eval_steps=100,\n",
    "    eval_strategy=\"steps\",\n",
    "\n",
    "    output_dir=os.environ[\"FULL_FT_MODEL_PATH\"],\n",
    "    save_total_limit=2,\n",
    "    save_strategy=\"epoch\",\n",
    "    \n",
    "    report_to=\"none\",\n",
    "    remove_unused_columns=False,\n",
    ")\n",
    "\n",
    "# Trainer class\n",
    "full_trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    args=train_args,\n",
    "    train_dataset=train_data,\n",
    "    eval_dataset=val_data,\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "\n",
    "# Before training\n",
    "torch.cuda.reset_peak_memory_stats()\n",
    "print(\"Allocated before training:\", torch.cuda.memory_allocated()/1e9, \"GB\")\n",
    "print(\"Reserved before training:\", torch.cuda.memory_reserved()/1e9, \"GB\")\n",
    "\n",
    "# Train Full Model with the Medical Q&A data.\n",
    "# After training, get peak memory usage\n",
    "full_trainer.train()\n",
    "\n",
    "print(\"Peak Allocated during training:\", torch.cuda.max_memory_allocated()/1e9, \"GB\")\n",
    "print(\"Peak Reserved during training:\", torch.cuda.max_memory_reserved()/1e9, \"GB\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c05f29e-49f9-4449-b98c-9dc1b138f286",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ab0e967d-6731-4313-8f22-52a05af78b24",
   "metadata": {},
   "source": [
    "---\n",
    "#### üíæ Step 4.1: Save Full Fine-Tuned Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fa5d8e0c-4a2a-44ec-bbc8-a1c2d67a733f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('/leonardo/home/userexternal/gcortiad/workshop-AddingKnowledgeToLLMs/FT-models/test/full_model_chatdoctor_gemma-2-2b-it/tokenizer_config.json',\n",
       " '/leonardo/home/userexternal/gcortiad/workshop-AddingKnowledgeToLLMs/FT-models/test/full_model_chatdoctor_gemma-2-2b-it/special_tokens_map.json',\n",
       " '/leonardo/home/userexternal/gcortiad/workshop-AddingKnowledgeToLLMs/FT-models/test/full_model_chatdoctor_gemma-2-2b-it/chat_template.jinja',\n",
       " '/leonardo/home/userexternal/gcortiad/workshop-AddingKnowledgeToLLMs/FT-models/test/full_model_chatdoctor_gemma-2-2b-it/tokenizer.model',\n",
       " '/leonardo/home/userexternal/gcortiad/workshop-AddingKnowledgeToLLMs/FT-models/test/full_model_chatdoctor_gemma-2-2b-it/added_tokens.json',\n",
       " '/leonardo/home/userexternal/gcortiad/workshop-AddingKnowledgeToLLMs/FT-models/test/full_model_chatdoctor_gemma-2-2b-it/tokenizer.json')"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# =====================================================\n",
    "#    4.1. Save Full Fine-Tuning Model\n",
    "# =====================================================\n",
    "# FT Full Model - ChatDoctor\n",
    "full_ft_model_chatdoctor = full_trainer.model\n",
    "\n",
    "# Save Full Model - ChatDoctor\n",
    "save_path_full_ft_model = os.getenv(\"FULL_FT_MODEL_PATH\", None)\n",
    "full_ft_model_chatdoctor.save_pretrained(save_path_full_ft_model)\n",
    "tokenizer.save_pretrained(save_path_full_ft_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db373758-34ac-4a9f-b0b4-64ebbcffde3c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4b6b1a37-7dbb-4eb2-bb70-7841d6821205",
   "metadata": {},
   "source": [
    "---\n",
    "#### üîÑ Step 5: Restart Kernel\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01261ab3-c72c-4e0a-867c-be16cb1c2280",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================================\n",
    "# 5. Restart Kernel \n",
    "# ========================================\n",
    "# Restart Kernel to clear cached objects and training artifacts\n",
    "# and to free GPU Memory (VRAM). This ensures a clean state for inference\n",
    "# and prevent Out-Of-Memory (OOM) errors.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90d0ab83-7627-482e-b61b-87c341f923c7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d8780cd5-dcee-4d87-b0e7-d5cd29864e7f",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### üîÆ Step 6: Inference with Base Model and Full FT Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "257a7f48-e3ff-4934-9178-f6990d15e640",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:01<00:00,  1.18it/s]\n",
      "Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:01<00:00,  1.07it/s]\n",
      "The module name  (originally ) is not a valid Python identifier. Please rename the original module to avoid import issues.\n"
     ]
    }
   ],
   "source": [
    "# =====================================================\n",
    "# 6. Inference with Base Model and Full FT Model\n",
    "# =====================================================\n",
    "# Import models alone\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from peft import PeftModel\n",
    "import torch\n",
    "from utils.utils import generate_chat_response\n",
    "import os\n",
    "\n",
    "# Define Environment Variables\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
    "os.environ[\"DATA_PATH\"] = \"/leonardo_work/tra26_minwinsc/workshop-AddingKnowledgeToLLMs/datasets/ChatDoctor-dataset/data/\"\n",
    "os.environ[\"MODEL_PATH\"] = \"/leonardo_work/tra26_minwinsc/workshop-AddingKnowledgeToLLMs/models/gemma-2-2b-it\"\n",
    "os.environ[\"FULL_FT_MODEL_PATH\"] = \"/leonardo_work/tra26_minwinsc/workshop-AddingKnowledgeToLLMs/FT-models/full_model_chatdoctor_gemma-2-2b-it/checkpoint-564/\"\n",
    "\n",
    "# Define path of the Base Model\n",
    "base_model_path = os.getenv(\"MODEL_PATH\", None)\n",
    "base_model_name = str(base_model_path.split(\"/\")[-1])\n",
    "\n",
    "# Define the path where Full FT Model is saved.\n",
    "save_path_full_ft_model = os.getenv(\"FULL_FT_MODEL_PATH\", None)\n",
    "\n",
    "# Read Base Model and Base Tokenizer\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    base_model_path,\n",
    "    torch_dtype=torch.bfloat16,    # Reduce GPU memory\n",
    "    device_map=\"auto\"              # Automatically put layers on GPU\n",
    ")\n",
    "base_tokenizer = AutoTokenizer.from_pretrained(base_model_path)\n",
    "\n",
    "\n",
    "# Read Full FT Model and Full FT Tokenizer\n",
    "full_model = AutoModelForCausalLM.from_pretrained(\n",
    "    save_path_full_ft_model,\n",
    "    torch_dtype=torch.bfloat16,    # Reduce GPU memory\n",
    "    device_map=\"auto\"              # Automatically put layers on GPU\n",
    ")\n",
    "full_tokenizer = AutoTokenizer.from_pretrained(save_path_full_ft_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "77f8c374-1b10-4d59-a41a-685c71afa8fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# How to do inference?\n",
    "#help(generate_chat_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daf7d2b9-9adb-4147-a6b0-5905d9693483",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### ‚úÖ Step 6.1: Inference with Base Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7cf7e10a-5a8d-4a49-bc62-3ced0babee07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I understand you're experiencing dizziness and nausea, and I'm sorry to hear you've been feeling unwell.  \n",
      "\n",
      "**I am an AI and cannot provide medical advice. The information below is for informational purposes only and does not constitute medical advice.**\n",
      "\n",
      "Based on your description, it sounds like you might be experiencing **vertigo**, which is a feeling of dizziness or spinning.  It's important to see a doctor to get a proper diagnosis and treatment plan. \n",
      "\n",
      "Here's why you should seek medical attention:\n",
      "\n",
      "* **Possible causes:** Your symptoms could be caused by various factors, including inner ear problems, migraines, low blood sugar, dehydration, or even medication side effects. \n",
      "* **Underlying conditions:**  Dizziness can sometimes be a symptom of a more serious underlying condition that requires medical attention.\n",
      "*  **Proper diagnosis:** A doctor can perform a physical exam, review your medical history, and potentially order tests to determine the cause of your symptoms.\n",
      "\n",
      "**What you can do in the meantime:**\n",
      "\n",
      "*  Stay hydrated: Drink plenty of fluids, especially water.\n",
      "   \n",
      "* Avoid caffeine and alcohol: These can worsen dizziness.\n",
      " \n",
      "**Do not:**\n",
      "\n",
      " * Drive or operate machinery: Dizziness can impair your ability to safely operate these.\n",
      " * Self-medicate:  Taking medication without a doctor's guidance can be dangerous.\n",
      "\n",
      "\n",
      "**Please schedule an appointment with your doctor or seek immediate medical attention if:**\n",
      "\n",
      "  * Your dizziness is severe or accompanied by other concerning symptoms like:\n",
      "    * Vision changes\n",
      "    \n",
      "    **Remember:**  It is crucial to consult a medical professional for a proper evaluation and treatment.\n"
     ]
    }
   ],
   "source": [
    "# =====================================================\n",
    "#    6.1. Inference with Base Model\n",
    "# =====================================================\n",
    "instruction = \"If you are a doctor, please answer the medical questions based on the patient's description.\"\n",
    "\n",
    "user_message = \"I woke up this morning feeling the whole room is spinning when i was sitting down. I went to the bathroom walking unsteadily, as i tried to focus i feel nauseous. I try to vomit but it wont come out.. After taking panadol and sleep for few hours, i still feel the same.. By the way, if i lay down or sit down, my head do not spin, only when i want to move around then i feel the whole world is spinning.. And it is normal stomach discomfort at the same time? Earlier after i relieved myself, the spinning lessen so i am not sure whether its connected or coincidences.. Thank you doc!\"\n",
    "user_message2 = \"Hello, My husband is taking Oxycodone due to a broken leg/surgery. He has been taking this pain medication for one month. We are trying to conceive our second baby. Will this medication afect the fetus? Or the health of the baby? Or can it bring birth defects? Thank you.\"\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": f\"INSTRUCTION:\\n{instruction}\\n\\nPATIENT MESSAGE:\\n{user_message}\"}\n",
    "]\n",
    "\n",
    "response = generate_chat_response(\n",
    "    messages=messages,\n",
    "    model=base_model,\n",
    "    tokenizer=base_tokenizer,\n",
    "    device=\"cuda\",\n",
    "    max_new_tokens=512,\n",
    "    temperature=0.2,\n",
    "    top_p=0.85,\n",
    "    top_k=50,\n",
    "    no_repeat_ngram_size=3,\n",
    ")\n",
    "\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "276e388a-56c5-42fb-9173-1503a8b0959f",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### üß™ Step 6.2: Inference with Full FT Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6f63003e-0237-4f7c-bfca-71847cd8eb2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hi, I think you may be having a problem called vertigo. In vertigo, the sensation of movement is experienced when a person is not actually moving. There are certain medical conditions that can cause vertigo, and the most common is benign positional paroxysmal vertigo (BPPV). BPPV is a condition that is completely reversible, and you may need treatment in the form of betahistine tablets. I would suggest that you consult your family physician for treatment. Hope I have answered your query. Let me know if I can assist you further.\n"
     ]
    }
   ],
   "source": [
    "# =====================================================\n",
    "#    6.2. Inference with Full FT Model\n",
    "# =====================================================\n",
    "response = generate_chat_response(\n",
    "    messages=messages,\n",
    "    model=full_model,\n",
    "    tokenizer=full_tokenizer,\n",
    "    device=\"cuda\",\n",
    "    max_new_tokens=512,\n",
    "    temperature=0.2,\n",
    "    top_p=0.85,\n",
    "    top_k=50,\n",
    "    no_repeat_ngram_size=3,\n",
    ")\n",
    "\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6ff1f5c-543b-4f4c-a68b-3a5b6f42eb85",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cf1e56d-019b-4b81-8171-ad03070fe81c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fine-tune-dev-display",
   "language": "python",
   "name": "fine-tune-dev"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
