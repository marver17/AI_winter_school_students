{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5f9605e7-b538-469c-b88d-4876c3fadd66",
   "metadata": {},
   "source": [
    "# üß† Workshop: Adding Knowledge to LLMs  \n",
    "### Dataset: lavita/ChatDoctor-HealthCareMagic-100k  \n",
    "HuggingFace: https://huggingface.co/datasets/lavita/ChatDoctor-HealthCareMagic-100k  \n",
    "\n",
    "### Base Model: google/gemma-2-2b-it  \n",
    "HuggingFace: https://huggingface.co/google/gemma-2-2b-it  \n",
    "\n",
    "---\n",
    "\n",
    "## 5Ô∏è‚É£ Create a Medical Agent\n",
    "\n",
    "In a **Medical Agent**, the Fine-Tuning to adapt the model to medical knowledge for high domain accuracy, and RAG (Retrieval-Augmented Generation) to supplement the model with up-to-date medical literature and tools.\n",
    "This combination enhances clinical relevance while maintaining safer, evidence-based responses.\n",
    "    \n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "42d77ebb-4ba0-4db0-a66f-70d304ee41f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Workshop: Adding Knowledge to LLMs\n",
    "# ============================================================\n",
    "# Dataset: lavita/ChatDoctor-HealthCareMagic-100k\n",
    "#         HuggingFace Dataset Link: https://huggingface.co/datasets/lavita/ChatDoctor-HealthCareMagic-100k\n",
    "\n",
    "# Model: google/gemma-2-2b-it\n",
    "#         HuggingFace Model Link: https://huggingface.co/google/gemma-2-2b-it\n",
    "\n",
    "# ============================================================\n",
    "# Goal:\n",
    "# - Fine-tune a model on Medical ChatDoctor Data using:\n",
    "# 1) Full Fine-Tuning\n",
    "# 2) LoRA\n",
    "# 3) QLoRA (4-bit + LoRA)\n",
    "# 4) Build a RAG baseline using the SAME data and Evaluate all approaches using the SAME questions\n",
    "# 5) Create a Medical Agent\n",
    "# ============================================================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b82f59cf-24fd-4a5c-8004-1db1185b08e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/leonardo/home/userexternal/gcortiad/workshop-AddingKnowledgeToLLMs/notebooks\n"
     ]
    }
   ],
   "source": [
    "# =====================================================\n",
    "# 5. Create a Medical Agent\n",
    "# =====================================================\n",
    "# Check Current Path\n",
    "!pwd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b59b9b9c-0d4b-40e0-82ca-b39611b869bf",
   "metadata": {},
   "source": [
    "‚ùó Remember:\n",
    "* **Tool calling is a feature** (tool calling is to agents what functions are to programs).\n",
    "* A function alone is not a program.\n",
    "* A tool alone is not an agent.\n",
    "* **Agent is an architecture**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f75de6e-b37b-471d-84de-e6b2db5e1d00",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### üì¶ Step 0: Deploy `google/gemma-2-2b-it` using vLLM on port 8000\n",
    "\n",
    "To be able to send requests to the model, we need to serve the `google/gemma-2-2b-it` model via an API server using the vLLM framework. The model will be running on port 8000, ready to handle incoming requests.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6dd216aa-34d3-4b0b-b6d8-6206065e9a1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Server started with PID 2555731\n"
     ]
    }
   ],
   "source": [
    "import subprocess\n",
    "\n",
    "# Comand to Serve google/gemma-2-2b-it Model using vLLM Framework in port 8000\n",
    "cmd = \"\"\"\n",
    "which /leonardo_work/tra26_minwinsc/workshop-AddingKnowledgeToLLMs/envs/vllm-env/bin/pip && /leonardo_work/tra26_minwinsc/workshop-AddingKnowledgeToLLMs/envs/vllm-env/bin/python3 -m vllm.entrypoints.openai.api_server --model /leonardo_work/tra26_minwinsc/workshop-AddingKnowledgeToLLMs/models/gemma-2-2b-it --dtype bfloat16 --enforce-eager --port 8000\n",
    "\"\"\"\n",
    "\n",
    "process = subprocess.Popen(\n",
    "    cmd,\n",
    "    shell=True,\n",
    "    executable=\"/bin/bash\",\n",
    "    stdout=open(\"server.log\", \"w\"),\n",
    "    stderr=open(\"server.err\", \"w\")\n",
    ")\n",
    "\n",
    "print(f\"Server started with PID {process.pid}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b8c57136-8220-4b8d-b1e0-53437addb92d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gcortiad 2555731 77.7  0.0 4416148 297912 ?      R    22:16   0:00 /leonardo_wor\n",
      "gcortiad 2555735  0.0  0.0   6500  1916 pts/0    S+   22:16   0:00 grep 2555731\n"
     ]
    }
   ],
   "source": [
    "# Is the process running?\n",
    "!ps aux | grep $process.pid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0325d7d6-0ee7-4d08-836f-33e749c5f2a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/leonardo_work/tra26_minwinsc/workshop-AddingKnowledgeToLLMs/envs/vllm-env/bin/pip\n"
     ]
    }
   ],
   "source": [
    "# Check output of the process\n",
    "!cat server.log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7c362547-6eb7-4cfe-8426-2779200d3812",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f6435a5-9b5d-44a0-908d-1cbf0bfc86e8",
   "metadata": {},
   "source": [
    "‚è≥ **Wait a few minutes for the model to fully load into the vLLM server**, then verify that it‚Äôs running by executing:\n",
    "\n",
    "```curl http://localhost:8000/v1/models```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c9fd7450-7340-4eac-ba10-e50aaa33e36f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"object\":\"list\",\"data\":[{\"id\":\"/leonardo_work/tra26_minwinsc/workshop-AddingKnowledgeToLLMs/models/gemma-2-2b-it\",\"object\":\"model\",\"created\":1771967869,\"owned_by\":\"vllm\",\"root\":\"/leonardo_work/tra26_minwinsc/workshop-AddingKnowledgeToLLMs/models/gemma-2-2b-it\",\"parent\":null,\"max_model_len\":8192,\"permission\":[{\"id\":\"modelperm-4c7fea78e72047cea6f2eb48f037fabe\",\"object\":\"model_permission\",\"created\":1771967869,\"allow_create_engine\":false,\"allow_sampling\":true,\"allow_logprobs\":true,\"allow_search_indices\":false,\"allow_view\":true,\"allow_fine_tuning\":false,\"organization\":\"*\",\"group\":null,\"is_blocking\":false}]}]}"
     ]
    }
   ],
   "source": [
    "# Check Healthy models in vLLM Server\n",
    "!curl http://localhost:8000/v1/models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49cb6191-cbed-4afc-850d-de62d6400a45",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### üì¶ Step 1: Environment Setup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3c31e1d3-a4ec-4cd6-bb2c-369e1ce82bcb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/venvs/llm/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# =====================================================\n",
    "# 1. Setup\n",
    "# =====================================================\n",
    "import os\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from datasets import load_dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer, AutoModelForCausalLM, TrainingArguments, Trainer,\n",
    "    DataCollatorForLanguageModeling, BitsAndBytesConfig\n",
    ")\n",
    "import math\n",
    "import faiss\n",
    "import requests\n",
    "import json\n",
    "import re\n",
    "import ast\n",
    "from typing import Dict, List\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from utils.utils import get_gpu_memory, generate_chat_response, generate_RAG_response, query_vllm, query_vllm_stream\n",
    "from asteval import Interpreter\n",
    "\n",
    "# langchain imports\n",
    "from langchain_classic.schema import Document\n",
    "from langchain.tools import tool\n",
    "from langchain_classic.vectorstores import FAISS\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.tools import Tool\n",
    "from langchain_classic.agents import initialize_agent, Tool\n",
    "from langchain_classic.chains import RetrievalQA\n",
    "from langchain_classic.schema import HumanMessage\n",
    "#from langchain_classic.chat_models import ChatOpenAI\n",
    "#from langchain.tools import tool\n",
    "\n",
    "\n",
    "# Define Environment Variables\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0,1,2,3\"\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
    "os.environ[\"PYTORCH_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
    "\n",
    "os.environ[\"DATA_PATH\"] = \"/leonardo_work/tra26_minwinsc/workshop-AddingKnowledgeToLLMs/datasets/ChatDoctor-dataset/data/\"\n",
    "os.environ[\"MODEL_PATH\"] = \"/leonardo_work/tra26_minwinsc/workshop-AddingKnowledgeToLLMs/models/gemma-2-2b-it\"\n",
    "os.environ[\"SAVE_PATH_FAISS_INDEX\"] = \"/leonardo_work/tra26_minwinsc/workshop-AddingKnowledgeToLLMs/FT-models/RAG/RAG-faiss-index.idx\"\n",
    "os.environ[\"SENTENCE_TRANSFORMER_MODEL_PATH\"]= \"/leonardo_work/tra26_minwinsc/workshop-AddingKnowledgeToLLMs/models/multi-qa-MiniLM-L6-cos-v1\"\n",
    "os.environ[\"QLORA_FT_MODEL_PATH\"] = \"/leonardo_work/tra26_minwinsc/workshop-AddingKnowledgeToLLMs/FT-models/QLoRA_model_chatdoctor_gemma-2-2b-it\"\n",
    "os.environ[\"LORA_FT_MODEL_PATH\"] = \"/leonardo_work/tra26_minwinsc/workshop-AddingKnowledgeToLLMs/FT-models/LoRA_model_chatdoctor_gemma-2-2b-it/\"\n",
    "os.environ[\"FULL_FT_MODEL_PATH\"] = \"/leonardo_work/tra26_minwinsc/workshop-AddingKnowledgeToLLMs/FT-models/full_model_chatdoctor_gemma-2-2b-it/checkpoint-564/\"\n",
    "os.environ[\"ROBERTA_LARGE_MODEL_PATH\"] = \"/leonardo_work/tra26_minwinsc/workshop-AddingKnowledgeToLLMs/models/roberta-large\"\n",
    "os.environ[\"SENTENCE_TRANSFORMER_SCORE_MODEL_PATH\"] = \"/leonardo_work/tra26_minwinsc/workshop-AddingKnowledgeToLLMs/models/all-mpnet-base-v2\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "eb836707-70b7-43c8-9647-68bfd2eec539",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'total_gb': 63.42, 'used_gb': 0.47, 'free_gb': 62.95, 'source': 'torch'}\n"
     ]
    }
   ],
   "source": [
    "gpu_mem = get_gpu_memory()\n",
    "print(gpu_mem)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0d56650-c17a-46cb-a30f-189e65574c29",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### üì• Step 2: Load Dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b865075d-3b5e-42ce-a821-21f8848fa445",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =====================================================\n",
    "# 2. Load ChatDoctor Dataset\n",
    "# =====================================================\n",
    "# Load the dataset from the local directory\n",
    "chatdoctor = load_dataset(os.getenv(\"DATA_PATH\", None))\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "549e316f-3f75-457d-b0ea-1bd54202f50e",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### üìÇ Step 3: Define Model Path and Load Tokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b3fee671-dd16-471e-8b84-6e7a666b5a81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =====================================================\n",
    "# 3. Model Path & Tokenizer\n",
    "# =====================================================\n",
    "# Define the model we want to fine tune.\n",
    "model_path = os.getenv(\"MODEL_PATH\", None)\n",
    "model_name = str(model_path.split(\"/\")[-1])\n",
    "\n",
    "# Get Model Tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f1ec5587-6acd-4e30-aeb5-8ffc493eb926",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model used for Medical Agent: gemma-2-2b-it\n"
     ]
    }
   ],
   "source": [
    "print(f\"Model used for Medical Agent: {model_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e17a8a8c-8c77-4960-8e66-e3855bdca565",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### üßπ Step 4: Apply Chat Template to the Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d8c4c4e0-5792-496f-8f3b-c605828e2409",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =====================================================\n",
    "# 4. Apply Chat Template to Data\n",
    "# =====================================================\n",
    "def format_chat_template(row):\n",
    "    row[\"text\"] = f\"PATIENT MESSAGE: {row['input']}\\nANSWER: {row['output']}\"\n",
    "    return row\n",
    "\n",
    "# Apply chat template to all data\n",
    "chatdoctor = chatdoctor.map(format_chat_template, num_proc=4)\n",
    "\n",
    "# Get train dataset\n",
    "train_dataset = chatdoctor['train']\n",
    "texts = [ex[\"text\"] for ex in train_dataset]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f681f61-e727-40f1-920d-d0f078a39ff4",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### üß© Step 5: Compute Embeddings\n",
    "\n",
    "Generate vector representations of your text to enable semantic search, retrieval, or other AI-powered tasks.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "663ebb95-515b-4f33-9090-4e270f12f407",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =====================================================\n",
    "# 5. Compute Embeddings\n",
    "# =====================================================\n",
    "# Define path of the Sentence Transformer Model (for Q&A detection).\n",
    "ST_model_path = os.getenv(\"SENTENCE_TRANSFORMER_MODEL_PATH\", None)\n",
    "\n",
    "# Load embedding model for detecting Q&A\n",
    "embed_model = SentenceTransformer(ST_model_path, device=device)\n",
    "\n",
    "# Encode all texts into numpy embeddings with a progress bar \n",
    "#embeddings = embed_model.encode(texts, convert_to_numpy=True, show_progress_bar=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ead2f93c-59a1-46da-acab-07b53db56f26",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### üóÇÔ∏è Step 6: Build FAISS Index\n",
    "Organize the embeddings into a **FAISS index** for fast similarity search and efficient retrieval.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d56a0220-46f2-45ac-abfd-27f56bafe3b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index loaded, total vectors: 112165\n"
     ]
    }
   ],
   "source": [
    "# =====================================================\n",
    "# 6. Build FAISS index\n",
    "# =====================================================\n",
    "# Load FAISS index from disk\n",
    "save_path_faiss_index = os.getenv(\"SAVE_PATH_FAISS_INDEX\", None)\n",
    "index = faiss.read_index(save_path_faiss_index)\n",
    "print(\"Index loaded, total vectors:\", index.ntotal)\n",
    "\n",
    "# Get embedding dimension.\n",
    "#embedding_dim = embeddings.shape[1]\n",
    "\n",
    "# Normalize embeddings for cosine similarity.\n",
    "#faiss.normalize_L2(embeddings)\n",
    "\n",
    "# Create FAISS index using inner product (cosine similarity after normalization).\n",
    "#index = faiss.IndexFlatIP(embedding_dim)\n",
    "\n",
    "# Add all embeddings to the index.\n",
    "#index.add(embeddings)\n",
    "\n",
    "# Confirm number of vectors in the index\n",
    "# print(\"FAISS index created with\", index.ntotal, \"vectors\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fe77281-1030-42fc-a24f-015c3b8e9d77",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### üìñ Step 7: Read Base Model - General Questions\n",
    "Use the base model (`google/gemma-2-2b-it`) to handle general-purpose queries before adding domain-specific knowledge.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "30c327a7-43c9-48cf-8ebd-814967ff89ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:01<00:00,  1.17it/s]\n"
     ]
    }
   ],
   "source": [
    "# =============================================================\n",
    "# 7. Read Base Model - General Questions\n",
    "# =============================================================\n",
    "# Read Base Model and Base Tokenizer\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_path,\n",
    "    torch_dtype=torch.bfloat16,    # Reduce GPU memory\n",
    "    device_map=\"auto\"             # Automatically put layers on GPU\n",
    ")\n",
    "base_tokenizer = AutoTokenizer.from_pretrained(model_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4625f303-970e-4bdd-b597-5ee9b8372f83",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### ü©∫ Step 8: Read LoRA FT Model\n",
    "Load the Fine-tuned LoRA model to handle **medical domain questions**, leveraging doctor-level Q&A knowledge.\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "08d7a547-17e6-4e4f-9d62-38708c0e920a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:01<00:00,  1.53it/s]\n"
     ]
    }
   ],
   "source": [
    "# =============================================================\n",
    "# 8. Read LoRA Model - FT LoRA Model with Medical Doctor Q&A\n",
    "# =============================================================\n",
    "# Define the path where LoRA FT Model is saved.\n",
    "save_path_lora_ft_model = os.getenv(\"LORA_FT_MODEL_PATH\", None)\n",
    "\n",
    "# Read LoRA FT Model and LoRA FT Tokenizer\n",
    "lora_model = AutoModelForCausalLM.from_pretrained(\n",
    "    save_path_lora_ft_model,\n",
    "    torch_dtype=torch.float16,    # Reduce GPU memory\n",
    "    device_map=\"auto\"             # Automatically put layers on GPU\n",
    ")\n",
    "lora_tokenizer = AutoTokenizer.from_pretrained(save_path_lora_ft_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3d2741f-2554-4c16-8dbb-9017bbab303e",
   "metadata": {},
   "source": [
    "Up to this point, we have loaded the Fine-tuned models and rebuilt the RAG database. Now, we are ready to **move on to the definition and setup of the Medical Agent.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2297759-0338-47cb-a44d-e542b724758d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f9232e4e-96db-43ff-8ca1-64b51bb1946b",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### ‚úÖ Step 9: Test Model - Verify LLM API Responses\n",
    "\n",
    "Check your `google/gemma-2-2b-it` model via the vLLM API and send a test query to ensure everything is working:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4584f96a-a647-4f8a-ae77-b61efb6e2a58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result: Let's talk about **AI applications in healthcare**. \n",
      "\n",
      "Here's a breakdown:\n",
      "\n",
      "**What is AI in Healthcare?**\n",
      "\n",
      "AI, or Artificial Intelligence, is the ability of computers to mimic human intelligence. In healthcare, AI is used to analyze large amounts of data, identify patterns, and make predictions to improve patient care, diagnosis, and treatment. \n",
      "\n",
      "**Examples of AI Applications in Healthcare:**\n",
      "\n",
      "* **Diagnosis and Screening:**\n",
      "    * **Image Analysis:** AI algorithms can analyze medical images (X-rays, CT scans, MRIs) to detect abnormalities like tumors, fractures, or infections with greater accuracy and speed than humans.\n",
      "    * **Predictive Analytics:** AI can analyze patient data (medical history, genetics, lifestyle) to predict the risk of developing certain diseases, allowing for early intervention.\n",
      "    * **Symptom Checkers:** AI-powered chatbots can help patients assess their symptoms and provide basic medical advice, reducing unnecessary doctor visits.\n",
      "\n",
      "* **Treatment and Drug Discovery:**\n",
      "    * **Personalized Medicine:** AI can analyze individual patient data to develop personalized treatment plans, optimizing drug dosages and minimizing side effects.\n",
      "    * **Drug Discovery:** AI can analyze vast datasets of chemical compounds to identify potential drug candidates for various diseases.\n",
      "    * **Robotic Surgery:** AI-assisted robots can perform complex surgeries with greater precision and less invasiveness.\n",
      "\n",
      "* **Administrative and Operational Efficiency:**\n",
      "    * **Appointment Scheduling:** AI can automate appointment scheduling, reducing wait times and improving patient flow.\n",
      "    * **Medical Billing and Coding:** AI can automate medical billing and coding, reducing errors and improving efficiency.\n",
      "    * **Patient Monitoring:** AI-powered wearables and sensors can monitor patients' vital signs, alerting healthcare providers to potential issues.\n",
      "\n",
      "\n",
      "**Benefits of AI in Healthcare:**\n",
      "\n",
      "* **Improved Accuracy and Efficiency:** AI can analyze vast amounts of data and identify patterns that humans might miss, leading to more accurate diagnoses and treatments.\n",
      "* **Personalized Care:** AI can tailor treatments to individual patients based on their unique characteristics and needs.\n",
      "* **Reduced Costs:** AI can automate tasks and improve efficiency, leading to lower healthcare costs.\n",
      "* **Increased Access to Care:** AI-powered tools can make healthcare more accessible to remote areas and underserved populations.\n",
      "\n",
      "**Challenges and Considerations:**\n",
      "\n",
      "* **Data Privacy and Security:** Protecting patient data is crucial, and ensuring ethical and responsible use of AI is essential.\n",
      "* **Bias and Fairness:** AI algorithms can inherit biases from the data they are trained on, leading to potential disparities in healthcare.\n",
      "* **Regulation and Oversight:** Establishing clear guidelines and regulations for AI in healthcare is necessary to ensure safety and efficacy.\n",
      "* **Human Oversight:** AI should be used as a tool to augment human expertise, not replace it.\n",
      "\n",
      "\n",
      "**Overall, AI is transforming healthcare in numerous ways, offering the potential to improve patient care, reduce costs, and increase access to care. However, it's crucial to address the challenges and ensure responsible development and implementation.** \n",
      "\n",
      "\n",
      "Let me know if you'd like to explore a specific aspect of AI in healthcare in more detail! \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# =====================================================\n",
    "# 9. Test Model - Verify LLM API Responses\n",
    "# =====================================================\n",
    "# Define vLLM Endpoint\n",
    "VLLM_ENDPOINT = \"http://localhost:8000/v1/chat/completions\"\n",
    "\n",
    "# Initialize `google/gemma-2-2b-it` model calls from local to vLLM API server\n",
    "llm = ChatOpenAI(\n",
    "    model=model_path,\n",
    "    openai_api_base=\"http://localhost:8000/v1\",\n",
    "    openai_api_key=\"sk-fake-key\",\n",
    "    temperature=0.3,\n",
    ")\n",
    "\n",
    "# Check that llm chat using vLLM framework is working correctly.\n",
    "result = llm.generate([[HumanMessage(content=\"Explain something about AI applications\")]])\n",
    "print(\"Result:\", result.generations[0][0].text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08f41425-b818-4176-8860-97ec013d47c6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "00303df9-2219-4f42-8bd3-82b809365730",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### üõ†Ô∏è Step 10: Define Tools Available for the Medical Agent\n",
    "\n",
    "Here, we define the **toolset that the Medical Agent will use**, specifying all capabilities it can leverage to handle medical queries safely and effectively."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dbb453e-22a6-43b4-b7e6-87d4914eddd0",
   "metadata": {},
   "source": [
    "#### Medical Agent  \n",
    "The **Medical Agent** is a specialized decision-support component designed to handle health-related queries.\n",
    " - Handles **medical queries** using specialized tools:  \n",
    "   - **BMI Calculator**  \n",
    "   - **BSA Calculator**  \n",
    "   - **Mock Illness Score (Rule-based LLM)**  \n",
    "   - **RAG Retrieval (Medical Data)**  \n",
    "   - **LoRA Fine-tuned Model (Medical Data)**\n",
    "\n",
    "![Medical Agent Architecture](images/MedicalAgent.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a47259e",
   "metadata": {},
   "source": [
    "```\n",
    "                         +-------------------+\n",
    "                         |   Medical Agent   |\n",
    "                         +-------------------+\n",
    "                                  |\n",
    "    -----------------------------------------------------------------\n",
    "    |               |                 |                |            |\n",
    "  (RAG)   (Body Surface Area)  (Fine-Tuned Model)  (Body Max Index)  (Mock Illness Score)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0903c41e-82d5-4357-8751-f1253c9ca3fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =====================================================\n",
    "#    Tools - BMI calculator\n",
    "# =====================================================\n",
    "def extract_height_weight_llm(input_text: str) -> dict:\n",
    "    \"\"\"\n",
    "    Uses VLLM to extract weight (kg) and height (m) from unstructured text.\n",
    "    Returns a dict: {\"weight_kg\": float | None, \"height_m\": float | None}\n",
    "    \"\"\"\n",
    "    system_prompt = \"\"\"\n",
    "    You are a medical data extraction engine.\n",
    "    \n",
    "    TASK:\n",
    "        Extract weight and height from unstructured text.\n",
    "        \n",
    "    RULES:\n",
    "        - Weight must be in kilograms (kg)\n",
    "        - Height must be in meters (m)\n",
    "        - Convert units if necessary (e.g. cm ‚Üí m)\n",
    "        - If a value is missing or unclear, return null\n",
    "        - NEVER ask questions\n",
    "    \n",
    "    OUTPUT JSON ONLY in this format:\n",
    "        {\n",
    "          \"weight_kg\": number or null,\n",
    "          \"height_m\": number or null\n",
    "        }\n",
    "    \"\"\".strip()\n",
    "\n",
    "    return query_vllm(system_prompt=system_prompt, user_prompt=input_text)\n",
    "\n",
    "    \n",
    "def calculate_bmi_llm(input_text: str) -> str:\n",
    "    \"\"\"\n",
    "    BMI (Body Mass Index): \n",
    "        Used for: Quick screening of underweight, normal, overweight, obesity.\n",
    "        \n",
    "    Calculates BMI by extracting height & weight using an LLM.\n",
    "        Input: single free-text string\n",
    "        Output: BMI string\n",
    "    \"\"\"\n",
    "\n",
    "    extracted = extract_height_weight_llm(input_text)\n",
    "    extracted = ast.literal_eval(extracted.replace(\"```\", \"\").replace(\"json\", \"\"))\n",
    "    weight_kg = extracted.get(\"weight_kg\", None)\n",
    "    height_m = extracted.get(\"height_m\", None)\n",
    "\n",
    "    if weight_kg is None or height_m is None or height_m <= 0:\n",
    "        return \"BMI could not be calculated: height or weight missing.\"\n",
    "\n",
    "    bmi = weight_kg / (height_m ** 2)\n",
    "\n",
    "    category = (\n",
    "        \"underweight\" if bmi < 18.5 else\n",
    "        \"normal\" if bmi < 25 else\n",
    "        \"overweight\" if bmi < 30 else\n",
    "        \"obese\"\n",
    "    )\n",
    "\n",
    "    return f\"BMI: {round(bmi, 2)} ({category})\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "89508a34-884f-4bde-8db5-ee9c07e52802",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =====================================================\n",
    "#    Tools - BSA calculator\n",
    "# =====================================================    \n",
    "def calculate_bsa_llm(input_text: str) -> str:\n",
    "    \"\"\"\n",
    "    BSA (Body Surface Area):\n",
    "    Used for:\n",
    "        * Medication dosing (especially chemo)\n",
    "        * Cardiac output indexing\n",
    "        * Better for medical dosing and physiological calculations.\n",
    "        \n",
    "    Calculates BSA (Body Surface Area) by extracting height & weight using an LLM.\n",
    "        Input: single free-text string\n",
    "        Output: BSA string\n",
    "    \"\"\"\n",
    "\n",
    "    # Extract height & weight using LLM\n",
    "    extracted = extract_height_weight_llm(input_text)\n",
    "\n",
    "    # If your extractor returns a stringified JSON, keep this\n",
    "    if isinstance(extracted, str):\n",
    "        extracted = ast.literal_eval(\n",
    "            extracted.replace(\"```\", \"\").replace(\"json\", \"\")\n",
    "        )\n",
    "\n",
    "    weight_kg = extracted.get(\"weight_kg\", None)\n",
    "    height_m = extracted.get(\"height_m\", None)\n",
    "\n",
    "    if (\n",
    "        not isinstance(weight_kg, (int, float)) or\n",
    "        not isinstance(height_m, (int, float)) or\n",
    "        height_m <= 0\n",
    "    ):\n",
    "        return \"BSA could not be calculated: height or weight missing.\"\n",
    "\n",
    "    # Convert height to cm\n",
    "    height_cm = height_m * 100\n",
    "\n",
    "    # Mosteller formula\n",
    "    bsa = math.sqrt((height_cm * weight_kg) / 3600)\n",
    "\n",
    "    return f\"BSA: {round(bsa, 2)} m¬≤\"\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6f4b987d-2628-42c4-a61d-fe87d460db07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =====================================================\n",
    "#    Tools - Check if symptoms are an emergency or not\n",
    "# =====================================================    \n",
    "def symptom_checker_llm(\n",
    "    input_text: str,\n",
    ") -> Dict:\n",
    "    \"\"\"\n",
    "    Function: \n",
    "        symptom_checker_llm\n",
    "\n",
    "    Description:\n",
    "        Check if the systems area a red flag or not and the patient needs immediate attention.\n",
    "        \n",
    "    Args:\n",
    "        input_text: User message\n",
    "        llm_call: function that takes (system_prompt, user_prompt) and returns parsed JSON\n",
    "\n",
    "    Returns:\n",
    "        boolean: Saying if it's a red-flag or not.\n",
    "        \n",
    "    \"\"\"\n",
    "\n",
    "    SYSTEM_PROMPT = \"\"\"\n",
    "    You are a medical triage assistant, not a doctor.\n",
    "    \n",
    "    Rules:\n",
    "        - Do NOT diagnose conditions.\n",
    "        - Do NOT provide treatment plans or medication dosing.\n",
    "        - Your role is to assess urgency and give general safety guidance.\n",
    "        - If symptoms suggest immediate danger, clearly advise seeking emergency care.\n",
    "        - Be calm, supportive, and concise.\n",
    "        - Output ONLY valid JSON matching the SymptomAdvice schema.\n",
    "        \n",
    "    Triage levels:\n",
    "        - emergency: symptoms that may be life-threatening\n",
    "        - urgent: symptoms that should be evaluated soon\n",
    "        - self_care: mild symptoms that can be monitored\n",
    "            \n",
    "    Always include:\n",
    "        - triage_level\n",
    "        - a brief summary\n",
    "        - practical next-step advice\n",
    "        - red flags detected (if any)\n",
    "        - follow-up questions to clarify risk\n",
    "    \"\"\"\n",
    "\n",
    "    response = query_vllm(\n",
    "        system_prompt=SYSTEM_PROMPT,\n",
    "        user_prompt=input_text,\n",
    "    )\n",
    "    return response\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "783751bd-1dfd-4f62-b613-fd21e758d814",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================================\n",
    "#     Tools - Illness Score using LLMs\n",
    "# ==========================================================\n",
    "def mock_illness_score_llm(input_text: str) -> str:\n",
    "    \"\"\"\n",
    "    Generates a mock illness severity score (0-100) using an LLM.\n",
    "    Input: single free-text string.\n",
    "    Output: string containing ONLY a number (agent-safe).\n",
    "    \"\"\"\n",
    "\n",
    "    system_prompt = \"\"\"\n",
    "    You are a medical triage scoring engine.\n",
    "    \n",
    "        RULES (MANDATORY):\n",
    "            - Infer severity ONLY from the provided text\n",
    "            - NEVER ask questions\n",
    "            - NEVER request more information\n",
    "            - ALWAYS return a score, even if information is limited\n",
    "            - If uncertain, choose a conservative mid-range score\n",
    "            \n",
    "        SCORING GUIDANCE:\n",
    "            - Higher scores for red-flag symptoms (chest pain, shortness of breath, fainting, confusion)\n",
    "            - Increase score for risk factors (high BMI, smoking, high cholesterol, older age)\n",
    "            - Ignore missing data\n",
    "        \n",
    "        OUTPUT RULES:\n",
    "            - Return ONLY a single integer between 0 and 100\n",
    "            - No words, no punctuation, no explanations\n",
    "            \n",
    "        VALID OUTPUT EXAMPLES:\n",
    "            42\n",
    "            78\n",
    "            15\n",
    "        \n",
    "        INVALID OUTPUT EXAMPLES:\n",
    "            \"Score: 42\"\n",
    "            42/100\n",
    "            The score is 42\n",
    "    \"\"\".strip()\n",
    "\n",
    "    raw_content = query_vllm(system_prompt=system_prompt, user_prompt=input_text)\n",
    "\n",
    "    # -----------------------------\n",
    "    # Safety net: extract a number\n",
    "    # -----------------------------\n",
    "    match = re.search(r\"\\b([0-9]{1,3})\\b\", raw_content)\n",
    "    if match:\n",
    "        score = int(match.group(1))\n",
    "        score = max(0, min(score, 100))  # clamp to 0‚Äì100\n",
    "        return str(score)\n",
    "\n",
    "    # Absolute fallback (agent-safe)\n",
    "    return \"50\"\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8ec601b4-43a7-4990-b276-1f1e6e424a1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================================\n",
    "#     Tools - RAG with Medical Q&A\n",
    "# ==========================================================\n",
    "def doctor_rag_tool(input_message: str) -> str:\n",
    "    \"\"\"\n",
    "    Doctor-style RAG tool.\n",
    "    Input: only the user's current message.\n",
    "    Uses global FAISS index, texts, embedding model, and query_vllm_stream.\n",
    "    \"\"\"\n",
    "\n",
    "    # -----------------------------\n",
    "    # 1. Embed the user query\n",
    "    # -----------------------------\n",
    "    query_embedding = embed_model.encode(\n",
    "        [input_message],\n",
    "        convert_to_tensor=True,\n",
    "        normalize_embeddings=True\n",
    "    ).cpu().numpy()\n",
    "\n",
    "    # -----------------------------\n",
    "    # 2. Retrieve top-k passages\n",
    "    # -----------------------------\n",
    "    top_k = 3\n",
    "    D, I = index.search(query_embedding, top_k)\n",
    "    retrieved_texts = [texts[i] for i in I[0]]\n",
    "    context_text = \"\\n\\n\".join(retrieved_texts)\n",
    "\n",
    "    # -----------------------------\n",
    "    # 3. Format prompt for LLM\n",
    "    # -----------------------------\n",
    "    prompt = f\"\"\"\n",
    "    You are a knowledgeable and empathetic doctor.\n",
    "    \n",
    "    Use the context below to answer the patient's question.\n",
    "    Provide clear, informative, and cautious guidance.\n",
    "    Do NOT prescribe medication or give emergency diagnosis; suggest seeing a professional if needed.\n",
    "    \n",
    "    CONTEXT:\n",
    "        {context_text}\n",
    "    \n",
    "    PATIENT QUESTION:\n",
    "        {input_message}\n",
    "    \n",
    "    ANSWER:\n",
    "    \"\"\"\n",
    "\n",
    "    # -----------------------------\n",
    "    # 4. Stream response from LLM\n",
    "    # -----------------------------\n",
    "    full_response = \"\"\n",
    "    for chunk in query_vllm_stream(prompt):\n",
    "        #print(chunk, end=\"\", flush=True)\n",
    "        full_response += chunk\n",
    "\n",
    "    return full_response\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ca5e8553-6e8d-49cc-a8bf-88f2cf448ab6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================================\n",
    "#     Tools - FT LoRA Model with Medical Q&A\n",
    "# ==========================================================\n",
    "def doctor_LoRAFTModel_tool(input_text: str) -> str:\n",
    "    \"\"\"\n",
    "    Agent Tool using a fine-tuned LoRA model for doctor-like responses.\n",
    "\n",
    "    Inputs:\n",
    "        - input_text: str, user question\n",
    "    Returns:\n",
    "        - model-generated response as string\n",
    "    \"\"\"\n",
    "\n",
    "    # Encode input\n",
    "    inputs = lora_tokenizer(input_text, return_tensors=\"pt\").to(lora_model.device)\n",
    "\n",
    "    # Generate output\n",
    "    with torch.no_grad():\n",
    "        outputs = lora_model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=512,\n",
    "            temperature=0.3,\n",
    "            do_sample=True,\n",
    "            pad_token_id=lora_tokenizer.eos_token_id\n",
    "        )\n",
    "\n",
    "    # Decode output\n",
    "    response = lora_tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    #print(\"response:\",response)\n",
    "    \n",
    "    return response\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f226fa0-0426-4975-a8bd-a0144eefb3cb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "59d3babd-a031-46d7-be4f-ccc3d22a8704",
   "metadata": {},
   "source": [
    "Once all the tools have been defined, we compile them into a toolset for the Medical Agent and then initialize the agent to start handling medical queries."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "222cf4a0-c707-43a7-9a1a-53eeb20f7810",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### ü§ñ Step 11: Initialize the Medical Agent\n",
    "With all tools defined and compiled, we now create the **Medical Agent**, enabling it to handle medical queries using the full toolset and RAG-enhanced knowledge base.\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "94ef841a-b2b4-4184-a693-36901dbcf4b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==============================\n",
      "QUESTION: Calculate BMI for 75kg and 1.80m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mThought: I need to use the BMI Calculator to calculate the BMI.\n",
      "Action: BMI Calculator\n",
      "Action Input: 75kg, 1.80m\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3mBMI: 23.15 (normal)\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mThought: I now know the final answer\n",
      "Final Answer: The BMI is 23.15. \n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "RESPONSE: The BMI is 23.15.\n",
      "\n",
      "==============================\n",
      "QUESTION: Check symptoms: chest pain, headache\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mThought: I need to check if the symptoms of chest pain and headache are a red flag.\n",
      "Action: Symptoms Checker\n",
      "Action Input: chest pain, headache\u001b[0m\n",
      "Observation: \u001b[38;5;200m\u001b[1;3m```json\n",
      "{\n",
      " \"triage_level\": \"urgent\",\n",
      " \"summary\": \"Chest pain and headache can be symptoms of serious conditions. It's important to get checked out by a medical professional.\",\n",
      " \"practical_next_step_advice\": \"Go to the nearest emergency room or call emergency services immediately.\",\n",
      " \"red_flags_detected\": \"Chest pain and headache are serious symptoms that require immediate medical attention.\",\n",
      " \"follow_up_questions\": \"Can you describe the chest pain? Is it sharp, stabbing, or crushing? Is it accompanied by shortness of breath, nausea, or dizziness?\"\n",
      "}\n",
      "```\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mThought: The symptoms checker indicates that chest pain and headache are serious and require immediate medical attention. \n",
      "Final Answer: The symptoms of chest pain and headache are serious and require immediate medical attention. \n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "RESPONSE: The symptoms of chest pain and headache are serious and require immediate medical attention.\n",
      "\n",
      "==============================\n",
      "QUESTION: Calculate BMI for 80kg, 1.8m and also check symptoms: shortness of breath, coughCalculate BMI and BSA for my that my weight is 80 kg and my height 1.80mI have sore throat for a couple of days and a slight fever\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mThought: I need to calculate the BMI first, then check the symptoms.\n",
      "Action: BMI Calculator\n",
      "Action Input: 80kg, 1.8m\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3mBMI: 24.69 (normal)\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mThought: I need to check the symptoms now.\n",
      "Action: Symptoms Checker\n",
      "Action Input: shortness of breath, cough, sore throat, slight fever\u001b[0m\n",
      "Observation: \u001b[38;5;200m\u001b[1;3m```json\n",
      "{\n",
      " \"triage_level\": \"urgent\",\n",
      " \"summary\": \"These symptoms could indicate a respiratory infection. It's important to get evaluated to determine the best course of action.\",\n",
      " \"practical_next_step_advice\": \"Contact your doctor or seek medical attention if symptoms worsen or you experience difficulty breathing.\",\n",
      " \"red_flags\": [],\n",
      " \"follow_up_questions\": [\"Have you had any recent exposure to someone with a cold or flu?\",\"Do you have any other symptoms, such as chest pain, dizziness, or confusion?\"]\n",
      "}\n",
      "```\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mThought: The BMI is normal, but the symptoms suggest a need for medical attention.\n",
      "Action: Symptoms Checker\n",
      "Action Input: shortness of breath, cough, sore throat, slight fever\u001b[0m\n",
      "Observation: \u001b[38;5;200m\u001b[1;3m```json\n",
      "{\n",
      "  \"triage_level\": \"urgent\",\n",
      "  \"summary\": \"Shortness of breath, cough, sore throat, and slight fever could indicate a respiratory infection. It's important to be evaluated for potential complications.\",\n",
      "  \"practical_next_step_advice\": \"Seek medical attention. A healthcare professional can assess your symptoms and provide appropriate treatment.\",\n",
      "  \"red_flags_detected\": \"Shortness of breath, fever\",\n",
      "  \"follow_up_questions\": [\n",
      "    \"Do you have any chest pain or pressure?\",\n",
      "    \"Have you experienced any difficulty breathing?\",\n",
      "    \"Have you been in contact with anyone who has been sick?\"\n",
      "  ]\n",
      "}\n",
      "```\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mThought:  The symptoms suggest a need for medical attention. \n",
      "Final Answer:  You should seek medical attention. \n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "RESPONSE: You should seek medical attention.\n"
     ]
    }
   ],
   "source": [
    "# =====================================================\n",
    "#   1Ô∏è‚É£ Listing all Tools\n",
    "# =====================================================\n",
    "TOOLS = [\n",
    "    Tool(\n",
    "        name=\"BMI Calculator\",\n",
    "        func=calculate_bmi_llm,\n",
    "        description=\"Calculates BMI (Body Mass Index: quick measure of body weight relative to height) from weight (kg) and height (m).\"\n",
    "    ),\n",
    "    Tool(\n",
    "        name=\"BSA Calculator\",\n",
    "        func=calculate_bsa_llm,\n",
    "        description=\"Calculates BSA (Body Surface Area: total surface area of the human body that is useful for drug dosing or othe rmedical calculations) from weight (kg) and height (cm).\"\n",
    "    ),\n",
    "    Tool(\n",
    "        name=\"Symptoms Checker\",\n",
    "        func=symptom_checker_llm,\n",
    "        description=\"Check if the symptoms of the patient are a red-flag or not and if the patient needs immediate attention.\"\n",
    "    ),\n",
    "    Tool(\n",
    "        name=\"Mock Illness Score LLM\",\n",
    "        func=mock_illness_score_llm,\n",
    "        description=\"Returns a mock illness severity score (0‚Äì100) using an LLM.\"\n",
    "    ),\n",
    "    Tool(\n",
    "        name=\"Doctor RAG Assistant\",\n",
    "        func=doctor_rag_tool,\n",
    "        description=\"Answers patient questions like a doctor using RAG (FAISS + LLM). Provides cautious guidance without prescribing or diagnosing.\"\n",
    "    ),\n",
    "    Tool(\n",
    "        name=\"Doctor FT LoRA Assistant\",\n",
    "        func=doctor_LoRAFTModel_tool,\n",
    "        description=\"Answers patient questions like a doctor using a fine-tuned LoRA model. Provides cautious guidance without prescribing or diagnosing.\"\n",
    "    )\n",
    "]\n",
    "\n",
    "# =====================================================\n",
    "#   2Ô∏è‚É£ Initialize the Medical Agent\n",
    "# =====================================================\n",
    "# Initialize the Medical Agent.\n",
    "medical_agent = initialize_agent(\n",
    "    tools=TOOLS,\n",
    "    llm=llm,\n",
    "    agent=\"zero-shot-react-description\",       # We want to check the observation, how Agent is reasoning and decision it took.\n",
    "    verbose=True                               # This will print out what the agent is doing at each step\n",
    ")\n",
    "\n",
    "# =====================================================\n",
    "#   3Ô∏è‚É£ Run Demo\n",
    "# =====================================================\n",
    "questions = [\n",
    "    \"Calculate BMI for 75kg and 1.80m\",\n",
    "    \"Check symptoms: chest pain, headache\",\n",
    "    \"Calculate BMI for 80kg, 1.8m and also check symptoms: shortness of breath, cough\"\n",
    "    \"Calculate BMI and BSA for my that my weight is 80 kg and my height 1.80m\"\n",
    "    \"I have sore throat for a couple of days and a slight fever\"\n",
    "]\n",
    "\n",
    "for q in questions:\n",
    "    print(\"\\n==============================\")\n",
    "    print(\"QUESTION:\", q)\n",
    "    response = medical_agent.run(q)\n",
    "    print(\"RESPONSE:\", response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eb90704-9eee-434f-a4a4-90d7350f0063",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0b0d4ca7-c1ee-4008-b0e0-04fddf3a4ff0",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### üåê Step 12: Initialize the General Agent\n",
    "In addition to the Medical Agent, we create a **General Agent** capable of handling non-medical queries, providing a broader AI assistant functionality.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fffb087-e168-4e72-ba4c-de373dd5090f",
   "metadata": {},
   "source": [
    "#### General Agent  \n",
    "The **General Agent** is a versatile decision-support component designed for broad-purpose queries.  \n",
    "- Handles **general queries** using:  \n",
    "   - **Calculator**  \n",
    "   - **General-purpose LLM** (open-ended questions, explanations, chit-chat)\n",
    "\n",
    "![General Agent Architecture](images/GeneralAgent.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b39b27a",
   "metadata": {},
   "source": [
    "```\n",
    "                 +----------------------+\n",
    "                 |     General Agent    |\n",
    "                 +----------------------+\n",
    "                        /          \\\n",
    "                       /            \\\n",
    "              (Calculator)        (Pure LLM)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "26c719eb-ed92-4f86-8e35-388d9a3d2e13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =====================================================\n",
    "#     Tools - Calculator Mathematical expressions\n",
    "# =====================================================\n",
    "# Define Calculator\n",
    "asteval_interpreter = Interpreter()\n",
    "\n",
    "def safe_calculator(input_str: str) -> str:\n",
    "    try:\n",
    "        result = asteval_interpreter(input_str)\n",
    "        if asteval_interpreter.error:\n",
    "            return \"Error in expression.\"\n",
    "        return str(result)\n",
    "    except Exception as e:\n",
    "        return f\"Exception: {e}\"\n",
    "\n",
    "\n",
    "calculator_tool = Tool(\n",
    "    name=\"Calculator\",\n",
    "    func=safe_calculator,\n",
    "    description=\"Evaluate mathematical expressions safely\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "dc117e9c-bf9f-46c2-ba03-60c4e19697dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =====================================================\n",
    "#     Tools - Pure LLM for General-purpose Q&A\n",
    "# =====================================================\n",
    "def pure_llm_tool(input_text: str) -> str:\n",
    "    \"\"\"\n",
    "    General-purpose LLM fallback tool.\n",
    "    Used for open-ended questions, explanations, chit-chat, etc.\n",
    "    \"\"\"\n",
    "    response = llm.invoke(input_text)\n",
    "    return response.content\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "e3950484-0cb7-4f9d-a179-325bd01b5ec1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==============================\n",
      "QUESTION: What is the capital of Italy?\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mThought: I can answer this question with a simple search.\n",
      "Action: Pure LLM\n",
      "Action Input: What is the capital of Italy?\u001b[0m\n",
      "Observation: \u001b[33;1m\u001b[1;3mThe capital of Italy is **Rome**. üáÆüáπ \n",
      "\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mThought: I now know the final answer\n",
      "Final Answer: The capital of Italy is Rome. üáÆüáπ \n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "RESPONSE: The capital of Italy is Rome. üáÆüáπ\n",
      "\n",
      "==============================\n",
      "QUESTION: 671218*38614\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mThought: I can use the Calculator tool to calculate this.\n",
      "Action: Calculator\n",
      "Action Input: 671218 * 38614\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3m25918411852\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mThought: I now know the final answer\n",
      "Final Answer: 25918411852 \n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "RESPONSE: 25918411852\n"
     ]
    }
   ],
   "source": [
    "# =====================================================\n",
    "#    12. General Agent\n",
    "# =====================================================\n",
    "# =====================================================\n",
    "#   1Ô∏è‚É£ Define General Tools List\n",
    "# =====================================================\n",
    "GENERAL_TOOLS = [\n",
    "    calculator_tool,\n",
    "    Tool(\n",
    "        name=\"Pure LLM\",\n",
    "        func=pure_llm_tool,\n",
    "        description=(\n",
    "            \"Use this tool for general questions, explanations, reasoning, \"\n",
    "            \"or when no other tool is appropriate.\"\n",
    "        )\n",
    "    )\n",
    "]\n",
    "\n",
    "# =====================================================\n",
    "#   2Ô∏è‚É£ Initialize the General Agent\n",
    "# =====================================================\n",
    "general_agent = initialize_agent(\n",
    "    tools=GENERAL_TOOLS,\n",
    "    llm=llm,\n",
    "    agent=\"zero-shot-react-description\",\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "# =====================================================\n",
    "#   3Ô∏è‚É£ Run Demo\n",
    "# =====================================================\n",
    "questions = [\n",
    "    \"What is the capital of Italy?\",\n",
    "    \"671218*38614\",\n",
    "]\n",
    "\n",
    "for q in questions:\n",
    "    print(\"\\n==============================\")\n",
    "    print(\"QUESTION:\", q)\n",
    "    response = general_agent.run(q)\n",
    "    print(\"RESPONSE:\", response)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "709b31c9-63b3-4095-8034-695e75e28005",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b8e339c4-6f36-4cec-8d65-b0c23bd7ef52",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### üß≠ Step 13: Master Router Agent\n",
    "Set up a **Router** to determine whether an incoming query should be handled by the **Medical Agent** or the **General Agent**, ensuring requests are routed to the most appropriate model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b86aa7d-aee9-4779-ada5-cd44f0108f24",
   "metadata": {},
   "source": [
    "#### Router Agent\n",
    "\n",
    "The **Router Agent** is a top-level dispatcher that determines the nature of an incoming query and directs it to the appropriate specialized agent.  \n",
    "- **Medical queries** ‚Üí routed to the **Medical Agent** for structured health reasoning.  \n",
    "- **General queries** ‚Üí routed to the **General Agent** for open-ended or numeric problem-solving.  \n",
    "\n",
    "This ensures that each question is handled by the agent best suited for its domain, enabling accurate and efficient responses.\n",
    "\n",
    "![Router Master Agent](images/RouterMasterAgent.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6691880",
   "metadata": {},
   "source": [
    "```\n",
    "                               +----------------------+\n",
    "                               |  Router Master Agent |\n",
    "                               +----------------------+\n",
    "                                          |\n",
    "                               +----------------------+\n",
    "                               | General or Medical?  |\n",
    "                               +----------------------+\n",
    "                                  /                \\\n",
    "                               General            Medical\n",
    "                                 /                    \\\n",
    "                +----------------------+     +----------------------+\n",
    "                |    General Agent     |     |     Medical Agent    |\n",
    "                +----------------------+     +----------------------+\n",
    "                      /         \\              /     |      |      |      \\\n",
    "               (Calculator)   (Pure LLM)   (RAG) (Body Surface Area) (Fine-Tuned Model) (Body Mass Index) (Mock Illness Score)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "09fad737-31f5-4f1e-91ce-6a980670f469",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =====================================================\n",
    "#    13. Router function\n",
    "# =====================================================\n",
    "def llm_router(query: str) -> str:\n",
    "    \"\"\"\n",
    "    Returns 'medical' or 'general' using an LLM.\n",
    "    \"\"\"\n",
    "    prompt = f\"\"\"\n",
    "    Classify the following question as either 'medical' or 'general':\n",
    "    Question: \"{query}\"\n",
    "        Only respond with 'medical' or 'general'.\n",
    "    \"\"\"\n",
    "    result = llm.invoke(prompt)\n",
    "    return result.content.strip().lower()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "40318ea7-71ae-4967-8182-424ebe26ea13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =====================================================\n",
    "#    Master Router Agent\n",
    "# =====================================================\n",
    "def master_agent_router(query: str):\n",
    "    decision=llm_router(query)\n",
    "    print(\"Decision:\", decision)\n",
    "    if decision == 'medical':\n",
    "        print(\"[Router] Sending to medical agent...\")\n",
    "        return medical_agent.run(query)\n",
    "    else:\n",
    "        print(\"[Router] Sending to general agent...\")\n",
    "        return general_agent.run(query)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "6f4d5712-b9c4-40fa-b934-563d1faf5432",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==============================\n",
      "QUESTION: Calculate BMI for 75kg and 1.80m\n",
      "Decision: medical\n",
      "[Router] Sending to medical agent...\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mThought: I need to use the BMI Calculator to calculate the BMI.\n",
      "Action: BMI Calculator\n",
      "Action Input: 75kg, 1.80m\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3mBMI: 23.15 (normal)\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mThought: I now know the BMI.\n",
      "Final Answer: The BMI is 23.15. \n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "RESPONSE: The BMI is 23.15.\n",
      "\n",
      "==============================\n",
      "QUESTION: What is 345 * 12?\n",
      "Decision: general\n",
      "[Router] Sending to general agent...\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mThought: I can use the Calculator tool to calculate this.\n",
      "Action: Calculator\n",
      "Action Input: 345 * 12\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3m4140\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mThought: I now know the final answer\n",
      "Final Answer: 4140 \n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "RESPONSE: 4140\n",
      "\n",
      "==============================\n",
      "QUESTION: I have sore throat for a couple of days and a slight fever\n",
      "Decision: medical\n",
      "[Router] Sending to medical agent...\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mThought: I should check if the symptoms are a red flag.\n",
      "Action: Symptoms Checker\n",
      "Action Input: I have sore throat for a couple of days and a slight fever\u001b[0m\n",
      "Observation: \u001b[38;5;200m\u001b[1;3m```json\n",
      "{\n",
      " \"triage_level\": \"urgent\",\n",
      " \"summary\": \"Sore throat and slight fever can be caused by various factors, including viral infections. It's important to get evaluated to determine the cause and receive appropriate treatment.\",\n",
      " \"practical_next_step_advice\": \"Contact your healthcare provider or seek medical attention if your symptoms worsen or you experience difficulty breathing, chest pain, or high fever.\",\n",
      " \"red_flags\": [],\n",
      " \"follow_up_questions\": [\"Have you experienced any other symptoms, such as cough, runny nose, or body aches?\"]\n",
      "}\n",
      "```\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mThought: I should ask the patient if they've experienced any other symptoms.\n",
      "Action: Symptoms Checker\n",
      "Action Input: I have sore throat for a couple of days and a slight fever\u001b[0m\n",
      "Observation: \u001b[38;5;200m\u001b[1;3m```json\n",
      "{\n",
      "  \"triage_level\": \"urgent\",\n",
      "  \"summary\": \"Sore throat and a slight fever could be caused by a viral infection. It's important to monitor your symptoms and seek medical attention if they worsen or you experience difficulty breathing, chest pain, or other concerning symptoms.\",\n",
      "  \"practical_next_step_advice\": \"You can try over-the-counter pain relievers like acetaminophen or ibuprofen to manage your fever and discomfort. Staying hydrated is also important. Consider resting and avoiding close contact with others to prevent spreading any potential infection.\",\n",
      "  \"red_flags\": [\n",
      "    \"Difficulty breathing\"\n",
      "  ],\n",
      "  \"follow_up_questions\": [\n",
      "    \"Have you experienced any other symptoms like cough, runny nose, or body aches?\"\n",
      "  ]\n",
      "}\n",
      "```\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mThought: The patient is experiencing sore throat and a slight fever.  The symptoms are concerning and could be a sign of a viral infection. It's important to monitor the symptoms and seek medical attention if they worsen. \n",
      "Final Answer: Given the symptoms of sore throat and a slight fever, it's important to monitor the situation and seek medical attention if the symptoms worsen or you experience difficulty breathing, chest pain, or other concerning symptoms. \n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "RESPONSE: Given the symptoms of sore throat and a slight fever, it's important to monitor the situation and seek medical attention if the symptoms worsen or you experience difficulty breathing, chest pain, or other concerning symptoms.\n",
      "\n",
      "==============================\n",
      "QUESTION: What is the capital of France?\n",
      "Decision: general\n",
      "[Router] Sending to general agent...\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mThought: I can answer this question using the Pure LLM tool.\n",
      "Action: Pure LLM\n",
      "Action Input: What is the capital of France?\u001b[0m\n",
      "Observation: \u001b[33;1m\u001b[1;3mThe capital of France is **Paris**. üá´üá∑ \n",
      "\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mThought: I now know the final answer\n",
      "Final Answer: Paris is the capital of France. \n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "RESPONSE: Paris is the capital of France.\n"
     ]
    }
   ],
   "source": [
    "# =====================================================\n",
    "#    Demo - Master Router Agent\n",
    "# =====================================================\n",
    "questions = [\n",
    "    \"Calculate BMI for 75kg and 1.80m\",\n",
    "    \"What is 345 * 12?\",\n",
    "    \"I have sore throat for a couple of days and a slight fever\",\n",
    "    \"What is the capital of France?\"\n",
    "]\n",
    "\n",
    "for q in questions:\n",
    "    print(\"\\n==============================\")\n",
    "    print(\"QUESTION:\", q)\n",
    "    response = master_agent_router(q)\n",
    "    print(\"RESPONSE:\", response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "1bdf198c-b3c9-4caf-ad91-4810afd9cb2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "question1 = \"What is the recommended first-line treatment for hypertension?\"\n",
    "question2 = \"Calculate BMI for a 70kg person with height 175cm\"\n",
    "question3 = \"I have chest pain and shortness of breath\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "4792ea0c-00c7-4e8c-97d9-ccceb43d9c8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==============================\n",
      "QUESTION: What is the recommended first-line treatment for hypertension?\n",
      "Decision: medical\n",
      "[Router] Sending to medical agent...\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mThought: I need to provide medical advice, so I should use the Doctor RAG Assistant. \n",
      "Action: Doctor RAG Assistant\n",
      "Action Input: What is the recommended first-line treatment for hypertension?\u001b[0m\n",
      "Observation: \u001b[33;1m\u001b[1;3m\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mThought: The Doctor RAG Assistant is designed to provide cautious guidance without prescribing or diagnosing. \n",
      "Action: Doctor RAG Assistant\n",
      "Action Input: What is the recommended first-line treatment for hypertension?\u001b[0m\n",
      "Observation: \u001b[33;1m\u001b[1;3m\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mThought: I need to provide medical advice, so I should use the Doctor RAG Assistant. \n",
      "Action: Doctor RAG Assistant\n",
      "Action Input: What is the recommended first-line treatment for hypertension?\u001b[0m\n",
      "Observation: \u001b[33;1m\u001b[1;3m\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mThought: I need to provide medical advice, so I should use the Doctor RAG Assistant. \n",
      "Action: Doctor RAG Assistant\n",
      "Action Input: What is the recommended first-line treatment for hypertension?\u001b[0m\n",
      "Observation: \u001b[33;1m\u001b[1;3m\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mThought: I need to provide medical advice, so I should use the Doctor RAG Assistant. \n",
      "Action: Doctor RAG Assistant\n",
      "Action Input: What is the recommended first-line treatment for hypertension?\u001b[0m\n",
      "Observation: \u001b[33;1m\u001b[1;3m\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mThought: I need to provide medical advice, so I should use the Doctor RAG Assistant. \n",
      "Action: Doctor RAG Assistant\n",
      "Action Input: What is the recommended first-line treatment for hypertension?\u001b[0m\n",
      "Observation: \u001b[33;1m\u001b[1;3m\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mThought: I need to provide medical advice, so I should use the Doctor RAG Assistant. \n",
      "Action: Doctor RAG Assistant\n",
      "Action Input: What is the recommended first-line treatment for hypertension?\u001b[0m\n",
      "Observation: \u001b[33;1m\u001b[1;3m\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mThought: I need to provide medical advice, so I should use the Doctor RAG Assistant. \n",
      "Action: Doctor RAG Assistant\n",
      "Action Input: What is the recommended first-line treatment for hypertension?\u001b[0m\n",
      "Observation: \u001b[33;1m\u001b[1;3m\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mThought: I need to provide medical advice, so I should use the Doctor RAG Assistant. \n",
      "Action: Doctor RAG Assistant\n",
      "Action Input: What is the recommended first-line treatment for hypertension?\u001b[0m\n",
      "Observation: \u001b[33;1m\u001b[1;3m\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mThought: I need to provide medical advice, so I should use the Doctor RAG Assistant. \n",
      "Action: Doctor RAG Assistant\n",
      "Action Input: What is the recommended first-line treatment for hypertension?\u001b[0m\n",
      "Observation: \u001b[33;1m\u001b[1;3m\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mThought: I need to provide medical advice, so I should use the Doctor RAG Assistant. \n",
      "Action: Doctor RAG Assistant\n",
      "Action Input: What is the recommended first-line treatment for hypertension?\u001b[0m\n",
      "Observation: \u001b[33;1m\u001b[1;3m\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mThought: I need to provide medical advice, so I should use the Doctor RAG Assistant. \n",
      "Action: Doctor RAG Assistant\n",
      "Action Input: What is the recommended first-line treatment for hypertension?\u001b[0m\n",
      "Observation: \u001b[33;1m\u001b[1;3m\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mThought: I need to provide medical advice, so I should use the Doctor RAG Assistant. \n",
      "Action: Doctor RAG Assistant\n",
      "Action Input: What is the recommended first-line treatment for hypertension?\u001b[0m\n",
      "Observation: \u001b[33;1m\u001b[1;3m\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mThought: I need to provide medical advice, so I should use the Doctor RAG Assistant. \n",
      "Action: Doctor RAG Assistant\n",
      "Action Input: What is the recommended first-line treatment for hypertension?\u001b[0m\n",
      "Observation: \u001b[33;1m\u001b[1;3m\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mThought: I need to provide medical advice, so I should use the Doctor RAG Assistant. \n",
      "Action: Doctor RAG Assistant\n",
      "Action Input: What is the recommended first-line treatment for hypertension?\u001b[0m\n",
      "Observation: \u001b[33;1m\u001b[1;3m\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3m\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "RESPONSE: Agent stopped due to iteration limit or time limit.\n",
      "\n",
      "==============================\n",
      "QUESTION: Calculate BMI for a 70kg person with height 175cm\n",
      "Decision: medical\n",
      "[Router] Sending to medical agent...\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mThought: I need to use the BMI Calculator to calculate the BMI.\n",
      "Action: BMI Calculator\n",
      "Action Input: 70kg, 175cm\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3mBMI: 22.86 (normal)\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mThought: I now have the BMI.\n",
      "Final Answer: The BMI is 22.86. \n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "RESPONSE: The BMI is 22.86.\n",
      "\n",
      "==============================\n",
      "QUESTION: I have chest pain and shortness of breath\n",
      "Decision: medical\n",
      "[Router] Sending to medical agent...\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mThought: This patient is reporting chest pain and shortness of breath, which are serious symptoms. I need to assess if these symptoms are a red flag and if the patient needs immediate attention.\n",
      "Action: Symptoms Checker\n",
      "Action Input: I have chest pain and shortness of breath\u001b[0m\n",
      "Observation: \u001b[38;5;200m\u001b[1;3m```json\n",
      "{\n",
      " \"triage_level\": \"emergency\",\n",
      " \"summary\": \"Chest pain and shortness of breath can be signs of a serious medical condition. Seek immediate medical attention.\",\n",
      " \"practical_next_step_advice\": \"Call emergency services (911 or your local equivalent) immediately.\",\n",
      " \"red_flags_detected\": \"Chest pain and shortness of breath are serious symptoms.\",\n",
      " \"follow_up_questions\": []\n",
      "}\n",
      "```\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mThought: This patient is experiencing serious symptoms and needs immediate medical attention.  I should not try to diagnose or treat them. \n",
      "Final Answer:  The patient should call emergency services immediately. \n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "RESPONSE: The patient should call emergency services immediately.\n"
     ]
    }
   ],
   "source": [
    "for q in [question1, question2, question3]:\n",
    "    print(\"\\n==============================\")\n",
    "    print(\"QUESTION:\", q)\n",
    "    response = master_agent_router(q)\n",
    "    print(\"RESPONSE:\", response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9b8f649-46a6-46c3-a996-b97085556d37",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bdb42eb-57c3-4e9a-812c-12cdc3b72707",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b72eac78-337d-4b8e-9930-f2093d0bca53",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LLM",
   "language": "python",
   "name": "llm"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
