{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5f9605e7-b538-469c-b88d-4876c3fadd66",
   "metadata": {},
   "source": [
    "# üß† Workshop: Adding Knowledge to LLMs  \n",
    "### Dataset: lavita/ChatDoctor-HealthCareMagic-100k  \n",
    "HuggingFace: https://huggingface.co/datasets/lavita/ChatDoctor-HealthCareMagic-100k  \n",
    "\n",
    "### Base Model: google/gemma-2-2b-it  \n",
    "HuggingFace: https://huggingface.co/google/gemma-2-2b-it  \n",
    "\n",
    "---\n",
    "\n",
    "## 3Ô∏è‚É£ QLoRA Qantized LoRA (Parameter-Efficient FT)\n",
    "\n",
    "In **QLoRA (Quantized Low-Rank Adaptation)**, we load the base model in **4-bit quantized format** and train low-rank adapter matrices on top of it, while keeping the original weights frozen.\n",
    "\n",
    "This dramatically reduces GPU memory usage compared to standard LoRA, enabling fine-tuning of large models on limited hardware while preserving strong domain adaptation performance.\n",
    "    \n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4643d928-67f0-4209-9f48-9dbd231a5089",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Workshop: Adding Knowledge to LLMs\n",
    "# ============================================================\n",
    "# Dataset: lavita/ChatDoctor-HealthCareMagic-100k\n",
    "#         HuggingFace Dataset Link: https://huggingface.co/datasets/lavita/ChatDoctor-HealthCareMagic-100k\n",
    "\n",
    "# Model: google/gemma-2-2b-it\n",
    "#         HuggingFace Model Link: https://huggingface.co/google/gemma-2-2b-it\n",
    "\n",
    "# ============================================================\n",
    "# Goal:\n",
    "# - Fine-tune a model on Medical ChatDoctor Data using:\n",
    "# 1) Full Fine-Tuning\n",
    "# 2) LoRA\n",
    "# 3) QLoRA (4-bit + LoRA)\n",
    "# 4) Build a RAG baseline using the SAME data and Evaluate all approaches using the SAME questions\n",
    "# 5) Create a Medical Agent\n",
    "# ============================================================\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "64c5bb70-7171-48ef-9828-e53037d2d50b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =====================================================\n",
    "# QLoRA Fine-Tuning\n",
    "# ====================================================="
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99b906ea-3d8f-413c-a9fe-b58083af5c1b",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### üì¶ Step 0: Environment Setup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "20a680a4-562d-45e4-ac54-c1edfbb31d2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/venvs/llm/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# =====================================================\n",
    "# 0. Setup\n",
    "# =====================================================\n",
    "import os\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer, AutoModelForCausalLM, TrainingArguments, Trainer,\n",
    "    DataCollatorForLanguageModeling, BitsAndBytesConfig\n",
    ")\n",
    "from peft import LoraConfig, get_peft_model\n",
    "from trl import SFTTrainer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from utils.utils import get_gpu_memory, generate_chat_response\n",
    "import bitsandbytes as bnb\n",
    "import torch.nn as nn\n",
    "from peft import prepare_model_for_kbit_training\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "64e9faa4-97e6-4f5c-9e13-a161035b2531",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Environment Variables\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
    "os.environ[\"DATA_PATH\"] = \"/leonardo_work/tra26_minwinsc/workshop-AddingKnowledgeToLLMs/datasets/ChatDoctor-dataset/data/\"\n",
    "os.environ[\"MODEL_PATH\"] = \"/leonardo_work/tra26_minwinsc/workshop-AddingKnowledgeToLLMs/models/gemma-2-2b-it\"\n",
    "os.environ[\"QLORA_FT_MODEL_PATH\"] = os.path.join(os.getenv(\"CINECA_SCRATCH\"), \"FT-models/scratch/QLoRA_model_chatdoctor_gemma-2-2b-it\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d0308113-3703-42bc-881b-c9c1431bc205",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bb6be30c-4271-441a-9f90-918c7223caf7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'total_gb': 63.42, 'used_gb': 0.47, 'free_gb': 62.95, 'source': 'torch'}\n"
     ]
    }
   ],
   "source": [
    "gpu_mem = get_gpu_memory()\n",
    "print(gpu_mem)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "286ac6b8-259b-44e6-8bfa-a34acba72787",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### üì• Step 1: Load Dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6a1cdec4-479d-491b-b3b0-3eb7b1464b3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =====================================================\n",
    "# 1. Load ChatDoctor Dataset\n",
    "# =====================================================\n",
    "# Load the dataset from the local directory\n",
    "chatdoctor = load_dataset(os.getenv(\"DATA_PATH\", None))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bc66b15f-b9e1-4455-8ecc-fecf36157a3e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['instruction', 'input', 'output'],\n",
       "        num_rows: 112165\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chatdoctor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9091cff5-c5f6-4f45-9eb6-570ae498a3f1",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### üìÇ Step 2: Define Model Path and Load Tokenizer\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "04ac816c-f6a8-4e70-8bf1-4efd27c94c50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =====================================================\n",
    "# 2. Tokenizer\n",
    "# =====================================================\n",
    "# Define the model we want to fine tune.\n",
    "model_path = os.getenv(\"MODEL_PATH\", None)\n",
    "model_name = str(model_path.split(\"/\")[-1])\n",
    "\n",
    "# Get Model Tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1b950771-3d7c-4005-ad93-77448c27fae8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model used for QLoRA Fine-Tuning: gemma-2-2b-it\n"
     ]
    }
   ],
   "source": [
    "print(f\"Model used for QLoRA Fine-Tuning: {model_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ea5f1ea-3547-4d25-8121-72f433009672",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### üßπ Step 3: Apply Chat Template to the Data + Tokenization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "421f5a30-3bf1-4ed5-9141-2ec839f53f6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =====================================================\n",
    "# 3. Apply Chat Template & Data Collator with Dynamic Padding\n",
    "# =====================================================\n",
    "def format_chat_template(row):\n",
    "    row_json = [{\"role\": \"user\", \"content\": f\"INSTRUCTION:\\n{row['instruction']}\\n\\nPATIENT MESSAGE:\\n{row['input']}\"},\n",
    "                {\"role\": \"assistant\", \"content\": row[\"output\"]}]\n",
    "    row[\"text\"] = tokenizer.apply_chat_template(row_json, tokenize=False)\n",
    "    return row\n",
    "\n",
    "# Apply chat template to all data\n",
    "chatdoctor = chatdoctor.map(format_chat_template, num_proc=1)\n",
    "\n",
    "# Get train dataset\n",
    "train_dataset = chatdoctor['train']\n",
    "\n",
    "# Define the Data Collator for creating batches of the data\n",
    "def data_collator(batch):\n",
    "    tokenized = tokenizer(\n",
    "        [example[\"text\"] for example in batch],\n",
    "        return_tensors=\"pt\",\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        max_length=2048,\n",
    "    )\n",
    "    # For causal LM, labels are just input_ids\n",
    "    tokenized[\"labels\"] = tokenized[\"input_ids\"].clone()\n",
    "\n",
    "    # Move everything to GPU 0.\n",
    "    #tokenized = {k: v.to('cuda:0') for k, v in tokenized.items()}\n",
    "    \n",
    "    return tokenized\n",
    "\n",
    "# Define the Data Collator for creating batches of the data\n",
    "def data_collator(batch):\n",
    "    tokenized = tokenizer(\n",
    "        [example[\"text\"] for example in batch],\n",
    "        return_tensors=\"pt\",\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        max_length=2048,\n",
    "    )\n",
    "    # For causal LM, labels are just input_ids\n",
    "    tokenized[\"labels\"] = tokenized[\"input_ids\"].clone()\n",
    "\n",
    "    # Move everything to GPU 0.\n",
    "    #tokenized = {k: v.to('cuda:0') for k, v in tokenized.items()}\n",
    "    \n",
    "    return tokenized\n",
    "\n",
    "\n",
    "# Subsample for workshop\n",
    "train_data = train_dataset.select(range(3000)) #.shuffle(seed=42).select(range(2000))\n",
    "#val_data = val_dataset.select(range(300))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2dbc1d7-ce42-403f-b1d3-e2fc19ddcfe9",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### ü§ñ Step 4: Load Gemma Model and Run the QLoRA Fine Tuning\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d3b546a7-4a41-4c5a-aaf3-94545e9f1536",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n",
      "Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:11<00:00,  5.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modules: ['q_proj', 'v_proj']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "PeftModelForCausalLM(\n",
       "  (base_model): LoraModel(\n",
       "    (model): Gemma2ForCausalLM(\n",
       "      (model): Gemma2Model(\n",
       "        (embed_tokens): Embedding(256000, 2304, padding_idx=0)\n",
       "        (layers): ModuleList(\n",
       "          (0-25): 26 x Gemma2DecoderLayer(\n",
       "            (self_attn): Gemma2Attention(\n",
       "              (q_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=2304, out_features=2048, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=2304, out_features=2, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=2, out_features=2048, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (k_proj): Linear4bit(in_features=2304, out_features=1024, bias=False)\n",
       "              (v_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=2304, out_features=1024, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=2304, out_features=2, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=2, out_features=1024, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (o_proj): Linear4bit(in_features=2048, out_features=2304, bias=False)\n",
       "            )\n",
       "            (mlp): Gemma2MLP(\n",
       "              (gate_proj): Linear4bit(in_features=2304, out_features=9216, bias=False)\n",
       "              (up_proj): Linear4bit(in_features=2304, out_features=9216, bias=False)\n",
       "              (down_proj): Linear4bit(in_features=9216, out_features=2304, bias=False)\n",
       "              (act_fn): GELUTanh()\n",
       "            )\n",
       "            (input_layernorm): Gemma2RMSNorm((2304,), eps=1e-06)\n",
       "            (post_attention_layernorm): Gemma2RMSNorm((2304,), eps=1e-06)\n",
       "            (pre_feedforward_layernorm): Gemma2RMSNorm((2304,), eps=1e-06)\n",
       "            (post_feedforward_layernorm): Gemma2RMSNorm((2304,), eps=1e-06)\n",
       "          )\n",
       "        )\n",
       "        (norm): Gemma2RMSNorm((2304,), eps=1e-06)\n",
       "        (rotary_emb): Gemma2RotaryEmbedding()\n",
       "      )\n",
       "      (lm_head): Linear(in_features=2304, out_features=256000, bias=False)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# =====================================================\n",
    "# 4. QLoRA (Quantized + LoRA)\n",
    "# =====================================================\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "torch.cuda.set_device(0)\n",
    "\n",
    "# Get Quantization configuration\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,                     # Load weights in 4-bit precision instead of the usual 16-bit\n",
    "    #bnb_4bit_compute_dtype=torch.bfloat16, # data type used for computations after quantization.\n",
    "    #bnb_4bit_use_double_quant=True,        # double quantization, a technique to reduce quantization error (re-quantized with a second small scale factor)\n",
    "    bnb_4bit_quant_type=\"nf4\"              # nf4 stands for NormalFloat 4-bit (nf4 uses nonlinear mapping)\n",
    ")\n",
    "\n",
    "# Base model quantized to 4-bit\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_path,\n",
    "    #load_in_4bit=True,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map={'': 0},\n",
    "    torch_dtype=torch.bfloat16  # optional for LoRA\n",
    ")\n",
    "\n",
    "modules = [\"q_proj\", \"v_proj\"]\n",
    "print(\"Modules:\", modules)\n",
    "\n",
    "peft_config = LoraConfig(\n",
    "    r=2,\n",
    "    lora_alpha=8,\n",
    "    target_modules=modules,\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")\n",
    "\n",
    "# Get LoRA Config in the Quantized Model\n",
    "model = get_peft_model(model, peft_config)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5c6695f0-7171-411c-a86c-ec8657bcaf20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modules: ['q_proj', 'v_proj']\n"
     ]
    }
   ],
   "source": [
    "modules = [\"q_proj\", \"v_proj\"]\n",
    "print(\"Modules:\", modules)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "02440ace-62db-4779-a90e-a9e119b82b37",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:accelerate.utils.other:Detected kernel version 4.18.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n",
      "The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'eos_token_id': 1}.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Allocated before training: 2.33914368 GB\n",
      "Reserved before training: 2.64241152 GB\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='72' max='72' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [72/72 10:09, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>4.110500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>48</td>\n",
       "      <td>3.946700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>72</td>\n",
       "      <td>3.753000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Peak Allocated during training: 46.381351936 GB\n",
      "Peak Reserved during training: 65.781366784 GB\n"
     ]
    }
   ],
   "source": [
    "# Define Training Arguments\n",
    "qlora_args = TrainingArguments(\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=4,\n",
    "    gradient_accumulation_steps=8,\n",
    "    \n",
    "    optim=\"paged_adamw_32bit\",\n",
    "    \n",
    "    output_dir=os.environ[\"QLORA_FT_MODEL_PATH\"],\n",
    "    save_total_limit=1,\n",
    "    save_strategy=\"epoch\",\n",
    "    \n",
    "    logging_strategy=\"epoch\",\n",
    "    warmup_steps=30,\n",
    "    \n",
    "    learning_rate=5e-5,\n",
    "    fp16=False,\n",
    "    bf16=False,\n",
    "\n",
    "    # System\n",
    "    report_to=\"none\",\n",
    "    remove_unused_columns=False,\n",
    "    dataloader_pin_memory=True,\n",
    "    dataloader_num_workers=2,\n",
    ")\n",
    "\n",
    "# Trainer class\n",
    "qlora_trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    args=qlora_args,\n",
    "    train_dataset=train_data,\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "\n",
    "# Before training\n",
    "torch.cuda.reset_peak_memory_stats()\n",
    "print(\"Allocated before training:\", torch.cuda.memory_allocated()/1e9, \"GB\")\n",
    "print(\"Reserved before training:\", torch.cuda.memory_reserved()/1e9, \"GB\")\n",
    "\n",
    "# Train qLoRA Model with the Medical Q&A data.\n",
    "# After training, get peak memory usage\n",
    "qlora_trainer.train()\n",
    "\n",
    "print(\"Peak Allocated during training:\", torch.cuda.max_memory_allocated()/1e9, \"GB\")\n",
    "print(\"Peak Reserved during training:\", torch.cuda.max_memory_reserved()/1e9, \"GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2ced7f70-28de-4c3d-8dc7-f8b42f915473",
   "metadata": {},
   "outputs": [],
   "source": [
    "#help(TrainingArguments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a71f3a5e-6a14-455c-a3fb-bd62daf24ec2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b39a1482-9bb7-4d33-871b-e93e05c5db31",
   "metadata": {},
   "source": [
    "---\n",
    "#### üíæ Step 4.1: Save QLoRA Fine-Tuned Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fa5d8e0c-4a2a-44ec-bbc8-a1c2d67a733f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('/leonardo_scratch/large/userexternal/gcortiad/FT-models/scratch/QLoRA_model_chatdoctor_gemma-2-2b-it/tokenizer_config.json',\n",
       " '/leonardo_scratch/large/userexternal/gcortiad/FT-models/scratch/QLoRA_model_chatdoctor_gemma-2-2b-it/special_tokens_map.json',\n",
       " '/leonardo_scratch/large/userexternal/gcortiad/FT-models/scratch/QLoRA_model_chatdoctor_gemma-2-2b-it/chat_template.jinja',\n",
       " '/leonardo_scratch/large/userexternal/gcortiad/FT-models/scratch/QLoRA_model_chatdoctor_gemma-2-2b-it/tokenizer.model',\n",
       " '/leonardo_scratch/large/userexternal/gcortiad/FT-models/scratch/QLoRA_model_chatdoctor_gemma-2-2b-it/added_tokens.json',\n",
       " '/leonardo_scratch/large/userexternal/gcortiad/FT-models/scratch/QLoRA_model_chatdoctor_gemma-2-2b-it/tokenizer.json')"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# =====================================================\n",
    "#    4.1. Save QLoRA Fine-Tuning Model\n",
    "# =====================================================\n",
    "# FT QLoRA Model - ChatDoctor\n",
    "qlora_model_chatdoctor = qlora_trainer.model\n",
    "\n",
    "# Save QLoRA Model - ChatDoctor\n",
    "save_path_qlora_ft_model = os.getenv(\"QLORA_FT_MODEL_PATH\", None)\n",
    "qlora_model_chatdoctor.save_pretrained(save_path_qlora_ft_model)\n",
    "tokenizer.save_pretrained(save_path_qlora_ft_model)\n",
    "#qlora_trainer.processing_class.save_pretrained(save_path_qlora_ft_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db373758-34ac-4a9f-b0b4-64ebbcffde3c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ba25bca6-3f1a-4e72-87af-76cc404b80d4",
   "metadata": {},
   "source": [
    "---\n",
    "#### üîÑ Step 5: Restart Kernel\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "01261ab3-c72c-4e0a-867c-be16cb1c2280",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================================\n",
    "# 5. Restart Kernel \n",
    "# ========================================\n",
    "# Restart Kernel to clear cached objects and training artifacts\n",
    "# and to free GPU Memory (VRAM). This ensures a clean state for inference\n",
    "# and prevent Out-Of-Memory (OOM) errors.use_gradient_checkpointing=True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90d0ab83-7627-482e-b61b-87c341f923c7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "cc6e26b9-1eec-4471-8068-212dc6203a0d",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### üîÆ Step 6: Inference with Base Model and QLoRA FT Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "257a7f48-e3ff-4934-9178-f6990d15e640",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:01<00:00,  1.16it/s]\n"
     ]
    }
   ],
   "source": [
    "# =====================================================\n",
    "# 6. Inference with Base Model and QLoRA FT Model\n",
    "# =====================================================\n",
    "# Import models alone\n",
    "import os\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from peft import PeftModel\n",
    "import torch\n",
    "from utils.utils import generate_chat_response\n",
    "from transformers import (\n",
    "    AutoTokenizer, AutoModelForCausalLM, TrainingArguments, Trainer,\n",
    "    DataCollatorForLanguageModeling, BitsAndBytesConfig\n",
    ")\n",
    "#os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "\n",
    "# Define Environment Variables\n",
    "os.environ[\"DATA_PATH\"] = \"/leonardo_work/tra26_minwinsc/workshop-AddingKnowledgeToLLMs/datasets/ChatDoctor-dataset/data/\"\n",
    "os.environ[\"MODEL_PATH\"] = \"/leonardo_work/tra26_minwinsc/workshop-AddingKnowledgeToLLMs/models/gemma-2-2b-it\"\n",
    "os.environ[\"QLORA_FT_MODEL_PATH\"] = \"/leonardo_work/tra26_minwinsc/workshop-AddingKnowledgeToLLMs/FT-models/QLoRA_model_chatdoctor_gemma-2-2b-it\"\n",
    "#\"/leonardo_scratch/large/userexternal/gcortiad/FT-models/scratch/QLoRA_model_chatdoctor_gemma-2-2b-it\"\n",
    "#os.path.join(\"/leonardo_work/tra26_minwinsc/workshop-AddingKnowledgeToLLMs/FT-models/QLoRA_model_chatdoctor_gemma-2-2b-it\")\n",
    "\n",
    "# Define path of the Base Model\n",
    "base_model_path = os.getenv(\"MODEL_PATH\", None)\n",
    "base_model_name = str(base_model_path.split(\"/\")[-1])\n",
    "\n",
    "# Define the path where Full FT Model is saved.\n",
    "save_path_qlora_ft_model = os.getenv(\"QLORA_FT_MODEL_PATH\", None)\n",
    "\n",
    "# Read Base Model and Base Tokenizer\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    base_model_path,\n",
    "    torch_dtype=torch.bfloat16,                     # Reduce GPU memory\n",
    "    device_map=\"auto\"                               # Automatically put layers on GPU\n",
    ")\n",
    "base_tokenizer = AutoTokenizer.from_pretrained(base_model_path)\n",
    "#base_tokenizer.pad_token = base_tokenizer.eos_token          # ensure padding token is set\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "77f8c374-1b10-4d59-a41a-685c71afa8fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# How to do inference?\n",
    "#help(generate_chat_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82b6f568-151c-4cec-8d22-2db69c18cb45",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### ‚úÖ Step 6.1: Inference with Base Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7cf7e10a-5a8d-4a49-bc62-3ced0babee07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I understand you're experiencing dizziness and nausea, and I'm sorry to hear you've been feeling unwell.  \n",
      "\n",
      "**It's important to understand that I am not a real doctor and cannot provide medical advice.** The information below is for general knowledge and should not be considered a substitute for professional medical advice. \n",
      "\n",
      "Based on your description, it sounds like you might be experiencing **vertigo**, which is a feeling of dizziness or spinning sensation.  Here are some possible causes:\n",
      "\n",
      "* **Benign Paroxysmal Positional Vertigo (BPPV):** This is a common cause of vertigo, where tiny calcium crystals in the inner ear become dislodged and cause the sensation of spinning. It's often triggered by changes in head position.\n",
      "* **Migraine:**  Some people experience vertigo as a symptom of migraines.\n",
      "  \n",
      "**What you should do:**\n",
      "\n",
      "1. **Seek immediate medical attention:**  It'd be best to see a doctor as soon as possible to rule out any serious underlying conditions. \n",
      "2. **Keep track of your symptoms:** Note down when the dizziness occurs, how long it lasts, and any other symptoms you experience. This information will be helpful for your doctor.\n",
      "3. **Avoid self-treating:**  While over-the-counter medications like Panadol can help with pain and discomfort, they won't address the underlying cause of your dizziness. \n",
      "\n",
      "\n",
      "**Possible next steps:**\n",
      "\n",
      "* Your doctor may perform a physical examination, review your medical history, and possibly order tests like an inner ear exam or imaging scans to determine the cause of the dizziness.\n",
      " * **Do not attempt to self-diagnose or self-medicate.** \n",
      "\n",
      "Remember, it's crucial to seek professional medical help for a proper diagnosis and treatment plan. \n",
      "\n",
      "\n",
      "\n",
      "Please take care and prioritize your health.\n"
     ]
    }
   ],
   "source": [
    "# =====================================================\n",
    "#    6.1. Inference with Base Model\n",
    "# =====================================================\n",
    "instruction = \"If you are a doctor, please answer the medical questions based on the patient's description.\"\n",
    "\n",
    "user_message = \"I woke up this morning feeling the whole room is spinning when i was sitting down. I went to the bathroom walking unsteadily, as i tried to focus i feel nauseous. I try to vomit but it wont come out.. After taking panadol and sleep for few hours, i still feel the same.. By the way, if i lay down or sit down, my head do not spin, only when i want to move around then i feel the whole world is spinning.. And it is normal stomach discomfort at the same time? Earlier after i relieved myself, the spinning lessen so i am not sure whether its connected or coincidences.. Thank you doc!\"\n",
    "user_message2 = \"Hello, My husband is taking Oxycodone due to a broken leg/surgery. He has been taking this pain medication for one month. We are trying to conceive our second baby. Will this medication afect the fetus? Or the health of the baby? Or can it bring birth defects? Thank you.\"\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": f\"INSTRUCTION:\\n{instruction}\\n\\nPATIENT MESSAGE:\\n{user_message}\"}\n",
    "]\n",
    "\n",
    "response = generate_chat_response(\n",
    "    messages=messages,\n",
    "    model=base_model,\n",
    "    tokenizer=base_tokenizer,\n",
    "    device=\"cuda\",\n",
    "    max_new_tokens=512,\n",
    "    temperature=0.2,\n",
    "    top_p=0.85,\n",
    "    top_k=50,\n",
    "    no_repeat_ngram_size=3,\n",
    ")\n",
    "\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dab08609-75dc-4acf-88e2-2c1c7145413f",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### üß™ Step 6.2: Inference with QLoRA FT Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6f63003e-0237-4f7c-bfca-71847cd8eb2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:02<00:00,  1.19s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hi, thanks for your query. I understand your concern. It seems you are suffering from vertigo. It is a condition where you feel dizzy and the room is rotating. It can be due to various reasons. The most common cause is inner ear problem. It may be due imbalance in inner ear or due to some infection. You need to consult an ENT specialist for proper diagnosis and treatment. He will examine you and may need some tests like MRI of the brain and inner ear. Treatment will be based on diagnosis. It will be mostly anti-vertigo medicines. You will need to take these medicines regularly for some time. You should avoid alcohol and smoking. Hope this answers your query, please feel free to ask further. Wish you good health.\n"
     ]
    }
   ],
   "source": [
    "# =====================================================\n",
    "#    6.2. Inference with QLoRA FT Model\n",
    "# =====================================================\n",
    "# Read QLoRA FT Model and QLoRA FT Tokenizer\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "qmodel = AutoModelForCausalLM.from_pretrained(\n",
    "    base_model_path,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"cuda:0\", # device_map=\"auto\",\n",
    ")\n",
    "\n",
    "# Read LoRA FT Model and LoRA FT Tokenizer\n",
    "qlora_model = PeftModel.from_pretrained(qmodel, save_path_qlora_ft_model)\n",
    "\n",
    "qlora_tokenizer = AutoTokenizer.from_pretrained(save_path_qlora_ft_model)\n",
    "\n",
    "response = generate_chat_response(\n",
    "    messages=messages,\n",
    "    model=qlora_model,\n",
    "    tokenizer=qlora_tokenizer,\n",
    "    device=\"cuda\",\n",
    "    max_new_tokens=512,\n",
    "    temperature=0.2,\n",
    "    top_p=0.85,\n",
    "    top_k=50,\n",
    "    no_repeat_ngram_size=3,\n",
    ")\n",
    "\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6ff1f5c-543b-4f4c-a68b-3a5b6f42eb85",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "11fe72ad-f990-486d-8035-d9e798d75400",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71f6eb85-d5e1-48d8-89e4-1e372998465d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c7ecc4a-ec3e-4bc0-a032-69e8230ddd16",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cf1e56d-019b-4b81-8171-ad03070fe81c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b72eac78-337d-4b8e-9930-f2093d0bca53",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LLM",
   "language": "python",
   "name": "llm"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
