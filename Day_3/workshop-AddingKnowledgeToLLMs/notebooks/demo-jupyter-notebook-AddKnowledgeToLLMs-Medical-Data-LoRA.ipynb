{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5f9605e7-b538-469c-b88d-4876c3fadd66",
   "metadata": {},
   "source": [
    "# üß† Workshop: Adding Knowledge to LLMs  \n",
    "### Dataset: lavita/ChatDoctor-HealthCareMagic-100k  \n",
    "HuggingFace: https://huggingface.co/datasets/lavita/ChatDoctor-HealthCareMagic-100k  \n",
    "\n",
    "### Base Model: google/gemma-2-2b-it  \n",
    "HuggingFace: https://huggingface.co/google/gemma-2-2b-it  \n",
    "\n",
    "---\n",
    "\n",
    "## 2Ô∏è‚É£ LoRA Fine-Tuning (Parameter-Efficient FT)\n",
    "\n",
    "In **LoRA (Low-Rank Adaptation)** Fine-Tuning, we freeze the base model weights and train small low-rank adapter matrices inserted into transformer layers.\n",
    "\n",
    "This significantly reduces memory usage while maintaining strong domain adaptation performance.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9becfc5c-2aae-469b-8729-64351410680d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Workshop: Adding Knowledge to LLMs\n",
    "# ============================================================\n",
    "# Dataset: lavita/ChatDoctor-HealthCareMagic-100k\n",
    "#         HuggingFace Dataset Link: https://huggingface.co/datasets/lavita/ChatDoctor-HealthCareMagic-100k\n",
    "\n",
    "# Model: google/gemma-2-2b-it\n",
    "#         HuggingFace Model Link: https://huggingface.co/google/gemma-2-2b-it\n",
    "\n",
    "# ============================================================\n",
    "# Goal:\n",
    "# - Fine-tune a model on Medical ChatDoctor Data using:\n",
    "# 1) Full Fine-Tuning\n",
    "# 2) LoRA\n",
    "# 3) QLoRA (4-bit + LoRA)\n",
    "# 4) Build a RAG baseline using the SAME data and Evaluate all approaches using the SAME questions\n",
    "# 5) Create a Medical Agent\n",
    "# ============================================================\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "64c5bb70-7171-48ef-9828-e53037d2d50b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =====================================================\n",
    "# LoRA Fine-Tuning\n",
    "# ====================================================="
   ]
  },
  {
   "cell_type": "markdown",
   "id": "897315b7-50b4-4aed-ad6f-e56a4b34fc3e",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### üì¶ Step 0: Environment Setup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "20a680a4-562d-45e4-ac54-c1edfbb31d2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/venvs/llm/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# =====================================================\n",
    "# 0. Setup\n",
    "# =====================================================\n",
    "import os\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer, AutoModelForCausalLM, TrainingArguments, Trainer,\n",
    "    DataCollatorForLanguageModeling, BitsAndBytesConfig\n",
    ")\n",
    "from peft import LoraConfig, get_peft_model\n",
    "from trl import SFTTrainer, SFTConfig\n",
    "from sklearn.model_selection import train_test_split\n",
    "from utils.utils import get_gpu_memory, generate_chat_response\n",
    "import bitsandbytes as bnb\n",
    "import torch.nn as nn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "64e9faa4-97e6-4f5c-9e13-a161035b2531",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Environment Variables\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
    "os.environ[\"DATA_PATH\"] = \"/leonardo_work/tra26_minwinsc/workshop-AddingKnowledgeToLLMs/datasets/ChatDoctor-dataset/data/\"\n",
    "os.environ[\"MODEL_PATH\"] = \"/leonardo_work/tra26_minwinsc/workshop-AddingKnowledgeToLLMs/models/gemma-2-2b-it\"\n",
    "os.environ[\"LORA_FT_MODEL_PATH\"] = \"/leonardo_work/tra26_minwinsc/workshop-AddingKnowledgeToLLMs/models/FT-models/test/LoRA_model_chatdoctor_gemma-2-2b-it\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d0308113-3703-42bc-881b-c9c1431bc205",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bb6be30c-4271-441a-9f90-918c7223caf7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'total_gb': 63.42, 'used_gb': 0.47, 'free_gb': 62.95, 'source': 'torch'}\n"
     ]
    }
   ],
   "source": [
    "gpu_mem = get_gpu_memory()\n",
    "print(gpu_mem)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b32206d-060a-4d1a-9126-8a47b2334b30",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### üì• Step 1: Load Dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6a1cdec4-479d-491b-b3b0-3eb7b1464b3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =====================================================\n",
    "# 1. Load ChatDoctor Dataset\n",
    "# =====================================================\n",
    "# Load the dataset from the local directory\n",
    "chatdoctor = load_dataset(os.getenv(\"DATA_PATH\", None))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9c3803b-2a1e-4b9c-a06d-f0d6d5973ae1",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### üìÇ Step 2: Define Model Path and Load Tokenizer\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "04ac816c-f6a8-4e70-8bf1-4efd27c94c50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =====================================================\n",
    "# 2. Tokenizer\n",
    "# =====================================================\n",
    "# Define the model we want to fine tune.\n",
    "model_path = os.getenv(\"MODEL_PATH\", None)\n",
    "model_name = str(model_path.split(\"/\")[-1])\n",
    "\n",
    "# Get Model Tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6c3c4bb3-1d27-4f36-ba6c-99f0ddff3083",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model used for LoRA Fine-Tuning: gemma-2-2b-it\n"
     ]
    }
   ],
   "source": [
    "print(f\"Model used for LoRA Fine-Tuning: {model_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5683e86d-f70e-407e-8435-5ac2d8a61d00",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### üßπ Step 3: Apply Chat Template to the Data + Tokenization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "421f5a30-3bf1-4ed5-9141-2ec839f53f6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =====================================================\n",
    "# 3. Apply Chat Template & Data Collator with Dynamic Padding\n",
    "# =====================================================\n",
    "def format_chat_template(row):\n",
    "    row_json = [\n",
    "        {\"role\": \"user\", \"content\": f\"INSTRUCTION:\\n{row['instruction']}\\n\\nPATIENT MESSAGE:\\n{row['input']}\"},\n",
    "        {\"role\": \"assistant\", \"content\": row[\"output\"]}\n",
    "    ]\n",
    "    row[\"text\"] = tokenizer.apply_chat_template(row_json, tokenize=False)\n",
    "    return row\n",
    "\n",
    "# Apply chat template to all data\n",
    "chatdoctor = chatdoctor.map(format_chat_template, num_proc=4)\n",
    "\n",
    "# Split Train and Test datasets\n",
    "split_dataset = chatdoctor['train'].train_test_split(\n",
    "    test_size=0.20,\n",
    "    seed=42,\n",
    "    shuffle=True,\n",
    ")\n",
    "train_dataset = split_dataset['train']\n",
    "val_dataset = split_dataset['test']\n",
    "\n",
    "\n",
    "# Define the Data Collator for creating batches of the data\n",
    "def data_collator(batch):\n",
    "    tokenized = tokenizer(\n",
    "        [example[\"text\"] for example in batch],\n",
    "        return_tensors=\"pt\",\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        max_length=2048,\n",
    "    )\n",
    "    # For causal LM, labels are just input_ids\n",
    "    tokenized[\"labels\"] = tokenized[\"input_ids\"].clone()\n",
    "    return tokenized\n",
    "\n",
    "# Subsample for workshop (select only X rows)\n",
    "train_data = train_dataset.select(range(3000)) #.shuffle(seed=42).select(range(2000)) # Shuffle before choosing X rows\n",
    "val_data = val_dataset.select(range(300))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "652a0a76-8550-462c-8c61-6d147d248703",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### ü§ñ Step 4: Load Gemma Model and Run the LoRA Fine Tuning\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "32206f0a-c215-4a31-ae10-738b6e30ea3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#help(TrainingArguments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d3b546a7-4a41-4c5a-aaf3-94545e9f1536",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n",
      "Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:01<00:00,  1.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modules: ['q_proj', 'v_proj']\n",
      "trainable params: 1,597,440 || all params: 2,615,939,328 || trainable%: 0.0611\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:accelerate.utils.other:Detected kernel version 4.18.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n",
      "The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'eos_token_id': 1}.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Allocated before training: 1.335625728 GB\n",
      "Reserved before training: 1.356857344 GB\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='282' max='282' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [282/282 08:28, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Entropy</th>\n",
       "      <th>Num Tokens</th>\n",
       "      <th>Mean Token Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>4.272900</td>\n",
       "      <td>4.124570</td>\n",
       "      <td>2.468862</td>\n",
       "      <td>791315.000000</td>\n",
       "      <td>0.346524</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>3.802000</td>\n",
       "      <td>4.061824</td>\n",
       "      <td>2.406161</td>\n",
       "      <td>1582630.000000</td>\n",
       "      <td>0.353356</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>3.749600</td>\n",
       "      <td>4.050017</td>\n",
       "      <td>2.392014</td>\n",
       "      <td>2373945.000000</td>\n",
       "      <td>0.355113</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Peak Allocated during training: 18.706855424 GB\n",
      "Peak Reserved during training: 65.020100608 GB\n"
     ]
    }
   ],
   "source": [
    "# =====================================================\n",
    "# 4. LoRA Fine-Tuning\n",
    "# =====================================================\n",
    "# Read LoRA Model\n",
    "lora_model = AutoModelForCausalLM.from_pretrained(model_path, torch_dtype=torch.bfloat16, device_map='auto')\n",
    "\n",
    "# Find Linear modules\n",
    "modules = [\"q_proj\", \"v_proj\"]\n",
    "print(\"Modules:\", modules)\n",
    "\n",
    "# Define LoRA Config\n",
    "lora_config = LoraConfig(\n",
    "    r=8,                        # Try distinct Rank values\n",
    "    lora_alpha=32,              # Try distinct lora_alpha values\n",
    "    target_modules=modules,\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")\n",
    "\n",
    "# Get only the LoRA params to modify during training\n",
    "lora_model = get_peft_model(lora_model, lora_config)\n",
    "lora_model.print_trainable_parameters()\n",
    "\n",
    "# Define Training Arguments\n",
    "lora_args = TrainingArguments(\n",
    "    # Throughput critical\n",
    "    per_device_train_batch_size=4,\n",
    "    gradient_accumulation_steps=8,\n",
    "\n",
    "    # Training length\n",
    "    num_train_epochs=3,\n",
    "\n",
    "    # Optimizer\n",
    "    learning_rate=5e-5, #2e-4,\n",
    "    fp16=False,\n",
    "    bf16=True,\n",
    "\n",
    "    # Loggingtrain_output\n",
    "    logging_strategy=\"epoch\",\n",
    "    warmup_steps=30,\n",
    "\n",
    "    output_dir=os.environ[\"LORA_FT_MODEL_PATH\"],\n",
    "    save_total_limit=1,\n",
    "    save_strategy=\"epoch\",\n",
    "\n",
    "    # Evaluation\n",
    "    eval_strategy=\"epoch\",\n",
    "    #eval_steps=50,\n",
    "    \n",
    "    # System\n",
    "    report_to=\"none\",\n",
    "    remove_unused_columns=False,\n",
    "    dataloader_pin_memory=True,\n",
    "    dataloader_num_workers=2,\n",
    ")\n",
    "\n",
    "# Trainer class\n",
    "lora_trainer = SFTTrainer(\n",
    "    model=lora_model,\n",
    "    args=lora_args,\n",
    "    train_dataset=train_data,\n",
    "    eval_dataset=val_data,\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "\n",
    "# Before training\n",
    "torch.cuda.reset_peak_memory_stats()\n",
    "print(\"Allocated before training:\", torch.cuda.memory_allocated()/1e9, \"GB\")\n",
    "print(\"Reserved before training:\", torch.cuda.memory_reserved()/1e9, \"GB\")\n",
    "\n",
    "# Train LoRA Model with the Medical Q&A data.\n",
    "# After training, get peak memory usage\n",
    "train_output = lora_trainer.train()\n",
    "\n",
    "print(\"Peak Allocated during training:\", torch.cuda.max_memory_allocated()/1e9, \"GB\")\n",
    "print(\"Peak Reserved during training:\", torch.cuda.max_memory_reserved()/1e9, \"GB\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9551ce39-da54-4359-8ec6-470910897f4b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=282, training_loss=3.9415091656624, metrics={'train_runtime': 510.4373, 'train_samples_per_second': 17.632, 'train_steps_per_second': 0.552, 'total_flos': 3.765156048298598e+16, 'train_loss': 3.9415091656624, 'epoch': 3.0})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20d98f8e-91b3-4a67-94d1-b8e30646c5b1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6aedb811-3788-428b-a0ef-543a8307a964",
   "metadata": {},
   "source": [
    "---\n",
    "#### üíæ Step 4.1: Save LoRA Fine-Tuned Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fa5d8e0c-4a2a-44ec-bbc8-a1c2d67a733f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('/leonardo_work/tra26_minwinsc/workshop-AddingKnowledgeToLLMs/models/FT-models/test/LoRA_model_chatdoctor_gemma-2-2b-it/tokenizer_config.json',\n",
       " '/leonardo_work/tra26_minwinsc/workshop-AddingKnowledgeToLLMs/models/FT-models/test/LoRA_model_chatdoctor_gemma-2-2b-it/special_tokens_map.json',\n",
       " '/leonardo_work/tra26_minwinsc/workshop-AddingKnowledgeToLLMs/models/FT-models/test/LoRA_model_chatdoctor_gemma-2-2b-it/chat_template.jinja',\n",
       " '/leonardo_work/tra26_minwinsc/workshop-AddingKnowledgeToLLMs/models/FT-models/test/LoRA_model_chatdoctor_gemma-2-2b-it/tokenizer.model',\n",
       " '/leonardo_work/tra26_minwinsc/workshop-AddingKnowledgeToLLMs/models/FT-models/test/LoRA_model_chatdoctor_gemma-2-2b-it/added_tokens.json',\n",
       " '/leonardo_work/tra26_minwinsc/workshop-AddingKnowledgeToLLMs/models/FT-models/test/LoRA_model_chatdoctor_gemma-2-2b-it/tokenizer.json')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# =====================================================\n",
    "# 4.1. LoRA Save Fine-Tuning Model\n",
    "# =====================================================\n",
    "# FT LoRA Model - ChatDoctor\n",
    "lora_ft_model_chatdoctor = lora_trainer.model\n",
    "\n",
    "# Save LoRA models - ChatDoctor\n",
    "save_path_lora_ft_model = os.getenv(\"LORA_FT_MODEL_PATH\", None)\n",
    "lora_ft_model_chatdoctor.save_pretrained(save_path_lora_ft_model, save_serialization=True)\n",
    "lora_trainer.tokenizer.save_pretrained(save_path_lora_ft_model)\n",
    "lora_trainer.processing_class.save_pretrained(save_path_lora_ft_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4254b45b-6348-4bdf-9089-51741c1446d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('/leonardo_work/tra26_minwinsc/workshop-AddingKnowledgeToLLMs/models/FT-models/test/LoRA_model_chatdoctor_gemma-2-2b-it/tokenizer_config.json',\n",
       " '/leonardo_work/tra26_minwinsc/workshop-AddingKnowledgeToLLMs/models/FT-models/test/LoRA_model_chatdoctor_gemma-2-2b-it/special_tokens_map.json',\n",
       " '/leonardo_work/tra26_minwinsc/workshop-AddingKnowledgeToLLMs/models/FT-models/test/LoRA_model_chatdoctor_gemma-2-2b-it/chat_template.jinja',\n",
       " '/leonardo_work/tra26_minwinsc/workshop-AddingKnowledgeToLLMs/models/FT-models/test/LoRA_model_chatdoctor_gemma-2-2b-it/tokenizer.model',\n",
       " '/leonardo_work/tra26_minwinsc/workshop-AddingKnowledgeToLLMs/models/FT-models/test/LoRA_model_chatdoctor_gemma-2-2b-it/added_tokens.json',\n",
       " '/leonardo_work/tra26_minwinsc/workshop-AddingKnowledgeToLLMs/models/FT-models/test/LoRA_model_chatdoctor_gemma-2-2b-it/tokenizer.json')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# FT LoRA Model - ChatDoctor\n",
    "\n",
    "# Save LoRA models - ChatDoctor\n",
    "save_path_lora_ft_model = os.getenv(\"LORA_FT_MODEL_PATH\", None)\n",
    "lora_model.save_pretrained(save_path_lora_ft_model, save_serialization=True)\n",
    "tokenizer.save_pretrained(save_path_lora_ft_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8b6ede4-e739-44ef-a28a-d104e6ada9c0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "231104e3-fc1b-450e-8e55-1152766e7e90",
   "metadata": {},
   "source": [
    "---\n",
    "#### üîÑ Step 5: Restart Kernel\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "01261ab3-c72c-4e0a-867c-be16cb1c2280",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================================\n",
    "# 5. Restart Kernel \n",
    "# ========================================\n",
    "# Restart Kernel to clear cached objects and training artifacts\n",
    "# and to free GPU Memory (VRAM). This ensures a clean state for inference\n",
    "# and prevent Out-Of-Memory (OOM) errors.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90d0ab83-7627-482e-b61b-87c341f923c7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2e465bc5-3c9f-4da7-ae5a-643dac9cb9d0",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### üîÆ Step 6: Inference with Base Model and LoRA FT Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "257a7f48-e3ff-4934-9178-f6990d15e640",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:01<00:00,  1.17it/s]\n"
     ]
    }
   ],
   "source": [
    "# =====================================================\n",
    "# 6. Inference with Base Model and FT LoRA Model\n",
    "# =====================================================\n",
    "# Import models alone\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from peft import PeftModel\n",
    "import torch\n",
    "from utils.utils import generate_chat_response\n",
    "import os\n",
    "\n",
    "#os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\"\n",
    "#os.environ[\"TORCH_USE_CUDA_DSA\"] = \"1\"\n",
    "#os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
    "os.environ[\"DATA_PATH\"] = \"/leonardo_work/tra26_minwinsc/workshop-AddingKnowledgeToLLMs/datasets/ChatDoctor-dataset/data/\"\n",
    "os.environ[\"MODEL_PATH\"] = \"/leonardo_work/tra26_minwinsc/workshop-AddingKnowledgeToLLMs/models/gemma-2-2b-it\"\n",
    "os.environ[\"LORA_FT_MODEL_PATH\"] = \"/leonardo_work/tra26_minwinsc/workshop-AddingKnowledgeToLLMs/FT-models/LoRA_model_chatdoctor_gemma-2-2b-it\"\n",
    "\n",
    "# Define path of the Base Model\n",
    "base_model_path = os.getenv(\"MODEL_PATH\", None)\n",
    "base_model_name = str(base_model_path.split(\"/\")[-1])\n",
    "\n",
    "# Define the path where LoRA FT Model is saved.\n",
    "save_path_lora_ft_model = os.getenv(\"LORA_FT_MODEL_PATH\", None)\n",
    "\n",
    "# Read Base Model and Base Tokenizer\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    base_model_path,\n",
    "    torch_dtype=torch.bfloat16,    # Reduce GPU memory\n",
    "    device_map=\"auto\"             # Automatically put layers on GPU\n",
    ")\n",
    "base_tokenizer = AutoTokenizer.from_pretrained(base_model_path)\n",
    "\n",
    "# Read LoRA FT Model and LoRA FT Tokenizer\n",
    "#lora_model = PeftModel.from_pretrained(base_model, save_path_lora_ft_model)\n",
    "\n",
    "#lora_tokenizer = AutoTokenizer.from_pretrained(save_path_lora_ft_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7d5cb66d-4b8f-4eb2-a1b7-7134404f0e8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#base_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dd59723b-6fae-40a1-97cb-955bb6795a14",
   "metadata": {},
   "outputs": [],
   "source": [
    "#lora_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "77f8c374-1b10-4d59-a41a-685c71afa8fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# How to do inference?\n",
    "#help(generate_chat_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3dea8f6-43cd-49c8-bdc2-63222b2c41d5",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### ‚úÖ Step 6.1: Inference with Base Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7cf7e10a-5a8d-4a49-bc62-3ced0babee07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I understand you're experiencing dizziness and nausea, and it's concerning. I'm an AI and cannot provide medical advice, but I can offer some information and suggest you seek immediate medical attention. \n",
      "\n",
      "**What you've described could be a sign of a serious condition, and you should not rely on self-diagnosis.**\n",
      "\n",
      "Here's why you need to see a doctor:\n",
      "\n",
      "* **Dizziness and Nausea:** These symptoms can be caused by various things, from dehydration to more serious issues like inner ear problems, migraines, or even a stroke. \n",
      "* **Spinning Sensation:**  The feeling of spinning is called vertigo, and can be a symptom of inner ear issues, migraines or even neurological problems.\n",
      "*  **Inability to Vomit:**  This could indicate a blockage in your digestive system or a more serious issue.\n",
      " \n",
      "**What to do:**\n",
      "\n",
      "1. **Seek immediate medical help:**  Call your doctor or go to the nearest emergency room. \n",
      "\n",
      "\n",
      "**Possible causes of your symptoms:**\n",
      "\n",
      "*  Inner ear problems (Benign Paroxysmal Positional Vertigo, Meniere's disease)\n",
      "* Migraines\n",
      "* Dehydration\n",
      "* Low blood sugar\n",
      "* Medication side effects\n",
      "* Anxiety or panic attacks\n",
      "* Inner ear infection\n",
      "* Stroke (in rare cases)\n",
      "\n",
      "\n",
      "**Important:**  Don't try to self-diagnose or self-treat.  A doctor can properly assess your symptoms, run tests if necessary, and provide the appropriate treatment. \n",
      "\n",
      "\n",
      "\n",
      "Please take care of yourself and seek medical attention as soon as possible.\n"
     ]
    }
   ],
   "source": [
    "# =====================================================\n",
    "# 6.1. Inference with Base Model\n",
    "# =====================================================\n",
    "instruction = \"If you are a doctor, please answer the medical questions based on the patient's description.\"\n",
    "\n",
    "user_message = \"I woke up this morning feeling the whole room is spinning when i was sitting down. I went to the bathroom walking unsteadily, as i tried to focus i feel nauseous. I try to vomit but it wont come out.. After taking panadol and sleep for few hours, i still feel the same.. By the way, if i lay down or sit down, my head do not spin, only when i want to move around then i feel the whole world is spinning.. And it is normal stomach discomfort at the same time? Earlier after i relieved myself, the spinning lessen so i am not sure whether its connected or coincidences.. Thank you doc!\"\n",
    "user_message2 = \"Hello, My husband is taking Oxycodone due to a broken leg/surgery. He has been taking this pain medication for one month. We are trying to conceive our second baby. Will this medication afect the fetus? Or the health of the baby? Or can it bring birth defects? Thank you.\"\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": f\"INSTRUCTION:\\n{instruction}\\n\\nPATIENT MESSAGE:\\n{user_message}\"}\n",
    "]\n",
    "\n",
    "response = generate_chat_response(\n",
    "    messages=messages,\n",
    "    model=base_model,\n",
    "    tokenizer=base_tokenizer,\n",
    "    device=\"cuda\",\n",
    "    max_new_tokens=512,\n",
    "    temperature=0.2,\n",
    "    top_p=0.85,\n",
    "    top_k=50,\n",
    "    no_repeat_ngram_size=3,\n",
    ")\n",
    "\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6a58ceb-22f4-4707-accf-7ea8b3741702",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5b13d304-bacf-4eab-adb3-7dc73cc0f6a1",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### üß™ Step 6.2: Inference with LoRA FT Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6f63003e-0237-4f7c-bfca-71847cd8eb2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hi, Thanks for your query on Chat Doctor. I have gone through your query and understand your concern. You have mentioned that you have been feeling nauseous and the room is feeling spinning. This could be due to a few reasons like inner ear problem, migraine, or even anxiety. I would suggest you to consult an ENT doctor to rule out any inner ear problems. If you are having migraine, then you can take a painkiller like paracetamol. If it is anxiety, then I would recommend you to take a stress management class. Hope this helps. Thanks.\n"
     ]
    }
   ],
   "source": [
    "# =====================================================\n",
    "# 6.2. Inference with LoRA FT Model\n",
    "# =====================================================\n",
    "# Read LoRA FT Model and LoRA FT Tokenizer\n",
    "lora_model = PeftModel.from_pretrained(base_model, save_path_lora_ft_model)\n",
    "\n",
    "lora_tokenizer = AutoTokenizer.from_pretrained(save_path_lora_ft_model)\n",
    "\n",
    "os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\"\n",
    "os.environ[\"TORCH_USE_CUDA_DSA\"] = \"1\"\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
    "\n",
    "response = generate_chat_response(\n",
    "    messages=messages,\n",
    "    model=lora_model,\n",
    "    tokenizer=lora_tokenizer,\n",
    "    device=\"cuda\",\n",
    "    max_new_tokens=512,\n",
    "    temperature=0.5,\n",
    "    top_p=0.85,\n",
    "    top_k=50,\n",
    "    no_repeat_ngram_size=3,\n",
    ")\n",
    "\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecd85da4-e9f2-4992-a367-bc0d52c41a8c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b81d648-ab19-440b-9010-51cc6665a736",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "46d8ac34-b05c-461a-b0f1-96bcdd1f4be9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello, Welcome to Chat Doctor. I have gone through your query and understand your concern. You are having nausea and dizziness. This could be due to various reasons. It could be anxiety, stress, dehydration, low blood sugar, low iron, or some other medical condition. It is important to rule out any serious medical condition first. I would suggest you to visit your doctor and get your blood sugar level checked. Also, get your iron level checked and get a complete blood count done. If you are having any other symptoms, please mention them. Hope I have answered your question. Let me know if I can assist you further. Take care.\n"
     ]
    }
   ],
   "source": [
    "# =====================================================\n",
    "# 6.2. Inference with LoRA FT Model\n",
    "# =====================================================\n",
    "# Read LoRA FT Model and LoRA FT Tokenizer\n",
    "lora_model = PeftModel.from_pretrained(base_model, save_path_lora_ft_model)\n",
    "\n",
    "lora_tokenizer = AutoTokenizer.from_pretrained(save_path_lora_ft_model)\n",
    "\n",
    "os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\"\n",
    "os.environ[\"TORCH_USE_CUDA_DSA\"] = \"1\"\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
    "\n",
    "response = generate_chat_response(\n",
    "    messages=messages,\n",
    "    model=lora_model,\n",
    "    tokenizer=lora_tokenizer,\n",
    "    device=\"cuda\",\n",
    "    max_new_tokens=512,\n",
    "    temperature=0.5,\n",
    "    top_p=0.85,\n",
    "    top_k=50,\n",
    "    no_repeat_ngram_size=3,\n",
    ")\n",
    "\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad0e5457-8995-4e08-9e82-4a9ad451e26e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23b7b85f-58b5-4729-9d66-1668a33ca4b4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a610be3f-de40-4a94-88d5-f1905ece274a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c674947b-fbb9-4acd-9bd3-db0afaeb2abe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a435495-207d-455d-8ccb-8093ba6aa05a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08ef994a-6dd7-4afa-b3a4-098fb9aa2127",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LLM",
   "language": "python",
   "name": "llm"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
